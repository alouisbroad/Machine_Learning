{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Assessement",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1Z3cdsVg-aaGccGYu_TrX0RE0jXt3y9qD",
      "authorship_tag": "ABX9TyPquU4/t8urjmomzuLqrMB7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alouisbroad/Machine_Learning/blob/main/Deep_Learning_Assessement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyNU33wWWDXv"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OPXtRpjizli"
      },
      "source": [
        "'''\n",
        "Deep Learning - Project\n",
        "\n",
        "Alistair Broad \n",
        "\n",
        "In the follow script I've used artificial neural networks to create a predictive model of \n",
        "whether breast cancer sufferers were likely to have recurrance. \n",
        "'''\n",
        "\n",
        "# Fixed dependencies - do not remove or change.\n",
        "import pytest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/gdrive/')\n",
        "# Import your dependencies\n",
        "\n",
        "# Dependencies \n",
        "import collections\n",
        "import datetime\n",
        "import keras\n",
        "from sklearn.preprocessing import OneHotEncoder # Encode the variables with more than one category that aren't ordinal. \n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras import Sequential\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "\n",
        "# Import data\n",
        "def import_local_data(file_path):\n",
        "    \"\"\"This function needs to import the data file into collab and return a pandas dataframe\n",
        "    \"\"\"\n",
        "    raw_df = pd.read_excel(file_path)\n",
        "\n",
        "\n",
        "    return raw_df\n",
        "\n",
        "\n",
        "local_file_path = '/content/drive/My Drive/Colab Notebooks/Data/breast-cancer.xls'\n",
        "\n",
        "# Dont change\n",
        "raw_data = import_local_data(local_file_path)\n",
        "\n",
        "\n",
        "# Exploring the Data\n",
        "\n",
        "# Inspect the dataset - gives simply view of the data set.\n",
        "print(\"Data looks like: \\n {}\".format(raw_data.head(10)), end =\"\\n \\n\")\n",
        "# Find the number of rows. \n",
        "print(\"The data has {} rows.\".format(len(raw_data.index)), end = \"\\n \\n\")\n",
        "# Iterate over the columns and inspect the unique values that occur and how many times they occur. \n",
        "for col in raw_data:  \n",
        "    print(\"Values in {} were:\".format(col), end = \"\\n\") \n",
        "    print(collections.Counter(raw_data[col]), end = \"\\n \\n\")\n",
        "\n",
        "\n",
        "print(\"Of the 286 observations, {}% of them were 'no-recurrence-events'.\".format(round((201/286)*100,1)), end = \"\\n \\n\")\n",
        "\n",
        "# Explain your key findings\n",
        "'''\n",
        "From a quick look at our data-set, we can see that there is 286 samples, which is a fairly small dataset for this sort of thing. \n",
        "We can also see that 70.3% were \"non-recurrance\" events, which may be important when considering/evaluating our final model. \n",
        "Now, looking at the data: \n",
        "Age - Median range is \"50-59\" and there's a skew towards older age ranges, \n",
        "Menopause - even though there's a skew towards older individuals, more were premenopause, \n",
        "tumor-size - to note here, \"10-14\" and \"5-9\" have been coverted in the source file to dates, \n",
        "inv-node - as with the above, \"3-5\", \"6-8\", \"9-11\" and \"12-14\" have also been converted to dates, \n",
        "node-caps - this has 8 missing data points and 78% answered \"no\",\n",
        "beast - this is fairly close to 50-50, as we would expect, \n",
        "breast-quad - this has 1 missing value.\n",
        "\n",
        "Also to note, is that the data contains a mix of data types (binary nominal, ordinal categorical etc.). \n",
        "\n",
        "We will need to find a suitable way to deal with missing values and encode the data so a solution can \n",
        "be found. \n",
        "'''\n",
        "\n",
        "# Split your data so that you can test the effectiveness of your model\n",
        "x = raw_data.iloc[:, :-1].values # Obtain the independent variables. \n",
        "y = raw_data.iloc[:, 9].values # Split out the dependent variable.\n",
        "\n",
        "# Encode the dependent variable.\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "\n",
        "# Creating the Training set and Test set\n",
        "# Here we take 25% of the data to test the model after learning from the other 80%.\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25) \n",
        "\n",
        "\n",
        "class Module4_Model:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        \n",
        "    def preprocess_training_data(self, training_df):\n",
        "        \"\"\"\n",
        "        This function should process the training data and store any features required in the class\n",
        "        \"\"\"\n",
        "        # Replacing Missing data\n",
        "        # For categorical data, we can delete row or replace with the mode.\n",
        "        # Below, I identify the required modes and sub in for missing values.\n",
        "\n",
        "        # Replacing missing values in \"node-caps\".\n",
        "        replace_missing = collections.Counter(training_df[:,4])            # This line counts the frequency of each entry.\n",
        "        replace_missing = replace_missing.most_common(1)[0][0]  # This finds the most frequent (mode).\n",
        "        training_df[:,4] = np.where(training_df[:,4] == '?', replace_missing, training_df[:,4]) # Applying the most common value to the missing ones. \n",
        "\n",
        "        # Replacing missing values in \"breast-quad\".\n",
        "        replace_missing = collections.Counter(training_df[:,7])            # This line counts the frequency of each entry.\n",
        "        replace_missing = replace_missing.most_common(1)[0][0]  # This finds the most frequent (mode).\n",
        "        training_df[:,7] = np.where(training_df[:,7] == '?', replace_missing, training_df[:,7]) # Applying the most common value to the missing ones. \n",
        "\n",
        "        training_df = pd.DataFrame(training_df) # Convert X to a dataframe to make the below easier.\n",
        "\n",
        "        # Encoding the Independent Variables\n",
        "        # ordinal categorical \n",
        "        age_mapping = {'20-29':1, '30-39':2, '40-49':3, '50-59':4, '60-69':5, '70-79':6}\n",
        "        training_df[0] = training_df[0].map(age_mapping)\n",
        "        tumorsize_mapping = { '0-4':1, datetime.datetime(2019, 9, 5, 0, 0):2, datetime.datetime(2014, 10, 1, 0, 0):3, '15-19':4, '20-24':5, '25-29':6, '30-34':7, '35-39':8, '40-44':9, '45-49':10, '50-54':11}\n",
        "        training_df[2] = training_df[2].map(tumorsize_mapping)\n",
        "        inv_nodes_mapping = {'0-2':1, datetime.datetime(2019, 5, 3, 0, 0):2, datetime.datetime(2019, 8, 6, 0, 0):3, datetime.datetime(2019, 11, 9, 0, 0):4, datetime.datetime(2014, 12, 1, 0, 0):5, '15-17':6, '24-26':7}\n",
        "        training_df[3] = training_df[3].map(inv_nodes_mapping)\n",
        "\n",
        "        # Nominal \n",
        "        node_caps_mapping = {\"no\":0, \"yes\":1}\n",
        "        training_df[4] = training_df[4].map(node_caps_mapping)\n",
        "        breast_mapping = {\"left\":0, \"right\":1}\n",
        "        training_df[6] = training_df[6].map(breast_mapping)\n",
        "        irradiat_mapping = {\"no\":0, \"yes\":1}\n",
        "        training_df[8] = training_df[8].map(irradiat_mapping)\n",
        "\n",
        "        # Encode the variables with more than one category that aren't ordinal. \n",
        "        ct = ColumnTransformer([('encoder', OneHotEncoder(), [1,7])], remainder='passthrough') \n",
        "        training_df = np.array(ct.fit_transform(training_df), dtype=np.float)\n",
        "        training_df = training_df[:, 1:] # Remove one dummy variable from menopause.\n",
        "        t = training_df[:, [2,0,1]]\n",
        "        training_df[:, [0,1,2]] = t\n",
        "        training_df = training_df[:, 1:] # Rearranging and removing 1 dummy variable for the \"breast-quad\" variable. \n",
        "\n",
        "        return training_df\n",
        "\n",
        "    def preprocess_test_data(self, test_df):\n",
        "        # Replacing Missing data\n",
        "        # For categorical data, we can delete row or replace with the mode.\n",
        "        # Below, I identify the required modes and sub in for missing values.\n",
        "\n",
        "        # Replacing missing values in \"node-caps\".\n",
        "        replace_missing = collections.Counter(test_df[:,4])            # This line counts the frequency of each entry.\n",
        "        replace_missing = replace_missing.most_common(1)[0][0]  # This finds the most frequent (mode).\n",
        "        test_df[:,4] = np.where(test_df[:,4] == '?', replace_missing, test_df[:,4]) # Applying the most common value to the missing ones. \n",
        "\n",
        "        # Replacing missing values in \"breast-quad\".\n",
        "        replace_missing = collections.Counter(test_df[:,7])            # This line counts the frequency of each entry.\n",
        "        replace_missing = replace_missing.most_common(1)[0][0]  # This finds the most frequent (mode).\n",
        "        test_df[:,7] = np.where(test_df[:,7] == '?', replace_missing, test_df[:,7]) # Applying the most common value to the missing ones. \n",
        "\n",
        "        test_df = pd.DataFrame(test_df) # Convert X to a dataframe to make the below easier.\n",
        "\n",
        "        # Encoding the Independent Variables\n",
        "        # ordinal categorical \n",
        "        age_mapping = {'20-29':1, '30-39':2, '40-49':3, '50-59':4, '60-69':5, '70-79':6}\n",
        "        test_df[0] = test_df[0].map(age_mapping)\n",
        "        tumorsize_mapping = { '0-4':1, datetime.datetime(2019, 9, 5, 0, 0):2, datetime.datetime(2014, 10, 1, 0, 0):3, '15-19':4, '20-24':5, '25-29':6, '30-34':7, '35-39':8, '40-44':9, '45-49':10, '50-54':11}\n",
        "        test_df[2] = test_df[2].map(tumorsize_mapping)\n",
        "        inv_nodes_mapping = {'0-2':1, datetime.datetime(2019, 5, 3, 0, 0):2, datetime.datetime(2019, 8, 6, 0, 0):3, datetime.datetime(2019, 11, 9, 0, 0):4, datetime.datetime(2014, 12, 1, 0, 0):5, '15-17':6, '24-26':7}\n",
        "        test_df[3] = test_df[3].map(inv_nodes_mapping)\n",
        "\n",
        "        # Nominal \n",
        "        node_caps_mapping = {\"no\":0, \"yes\":1}\n",
        "        test_df[4] = test_df[4].map(node_caps_mapping)\n",
        "        breast_mapping = {\"left\":0, \"right\":1}\n",
        "        test_df[6] = test_df[6].map(breast_mapping)\n",
        "        irradiat_mapping = {\"no\":0, \"yes\":1}\n",
        "        test_df[8] = test_df[8].map(irradiat_mapping)\n",
        "\n",
        "        # Encode the variables with more than one category that aren't ordinal. \n",
        "        ct = ColumnTransformer([('encoder', OneHotEncoder(), [1,7])], remainder='passthrough') \n",
        "        test_df = np.array(ct.fit_transform(test_df), dtype=np.float)\n",
        "        test_df = test_df[:, 1:] # Remove one dummy variable from menopause.\n",
        "        t = test_df[:, [2,0,1]]\n",
        "        test_df[:, [0,1,2]] = t\n",
        "        test_df = test_df[:, 1:] # Rearranging and removing 1 dummy variable for the \"breast-quad\" variable. \n",
        "\n",
        "        return test_df\n",
        "    \n",
        "\n",
        "# Dont change\n",
        "my_model = Module4_Model()\n",
        "\n",
        "# Dont change\n",
        "x_train_processed = my_model.preprocess_training_data(x_train)\n",
        "\n",
        "#### Model 1 #### \n",
        "\n",
        "# Create a model\n",
        "\n",
        "# Initialising the ANN\n",
        "classifier = Sequential() # model class\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "# add method used to add layers.\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13)) \n",
        "\n",
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu')) \n",
        "\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid')) # Sigmoid is used at the end to get probability at the end.\n",
        "# for more dependent variables with more than two categories use  - change units to the number you have and the activation to a multiple sigmoid version \"softmax\".\n",
        "\n",
        "# Compiling the ANN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) \n",
        "# adam is a stocastic gradient decent algorithm\n",
        "# for more dependent variables with more than two categories use \"category_crossentropy\"\n",
        "\n",
        "\n",
        "# Dont change\n",
        "x_test_processed = my_model.preprocess_test_data(x_test)\n",
        "\n",
        "\n",
        "# Train your model\n",
        "# Fitting the ANN to the Training set\n",
        "classifier.fit(x_train_processed, y_train, batch_size = 10, epochs = 100)\n",
        "\n",
        "# use your model to make a prediction on unseen data\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(x_test_processed)\n",
        "y_pred = (y_pred > 0.5) # Threshold of 50% \n",
        "\n",
        "# Asssess the accuracy of your model and explain your key findings\n",
        "# Making the Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(cm)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\n",
        "print(\"Accuracy: {} \\n \\n\".format(accuracy))\n",
        "# Precision\n",
        "precision = (cm[1][1])/(cm[1][1]+cm[0][1])\n",
        "print(\"Precision: {} \\n \\n\".format(precision))\n",
        "# Recall \n",
        "recall = (cm[1][1])/(cm[1][1]+cm[1][0])\n",
        "print(\"Recall: {} \\n \\n\".format(recall))\n",
        "# F1 Score\n",
        "print(\"F1 Score: {} \\n \\n\".format((precision*recall)/(recall+precision)))\n",
        "\n",
        "'''\n",
        "OUTPUT:\n",
        "[[46  7]\n",
        " [15  4]]\n",
        "Accuracy: 0.6944444444444444 \n",
        " \n",
        "\n",
        "Precision: 0.36363636363636365 \n",
        " \n",
        "\n",
        "Recall: 0.21052631578947367 \n",
        " \n",
        "\n",
        "F1 Score: 0.13333333333333333 \n",
        "\n",
        "Looking over these outputs, we can see that this is currently a pretty terrible model. Percision, recall and f1 are exceptionally low\n",
        "and accuracy is being below what would be achieved if the model always predicted \"non-recurrance\". Below I will try to impove the \n",
        "model by searching for the best parameters and dropout (which should help to prevent overfitting).\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoyF_0G0wMWG"
      },
      "source": [
        "'''\n",
        "Parameter tuning with gridsearch \n",
        "Note: This section takes an exceptionally long time to run - hence, why I only used 200 epochs.\n",
        "\n",
        "Here I've tested if having more epochs, batch size or a different optimiser will give us a \n",
        "better model. \n",
        "'''\n",
        "\n",
        "# Tuning the ANN\n",
        "def build_classifier(optimizer):\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13))\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    return classifier\n",
        "classifier = KerasClassifier(build_fn = build_classifier)\n",
        "parameters = {'batch_size': [25, 32],\n",
        "              'epochs': [100, 200],\n",
        "              'optimizer': ['adam', 'rmsprop']}\n",
        "grid_search = GridSearchCV(estimator = classifier,\n",
        "                           param_grid = parameters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 10)\n",
        "grid_search = grid_search.fit(x_train_processed , y_train)\n",
        "best_parameters = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "\n",
        "print(best_accuracy)\n",
        "print(best_parameters)\n",
        "\n",
        "'''\n",
        "OUTPUT: \n",
        "0.7146245059288537\n",
        "{'batch_size': 32, 'epochs': 100, 'optimizer': 'adam'}\n",
        "\n",
        "So here we have found the the number of epochs and optimiser we'd chosen was the best, but this \n",
        "suggests that increasing batch size will give a marginally better result - the accuracy is now\n",
        "at least above the percentage of \"non-reccurance\".\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa_Lz5CrwFJk"
      },
      "source": [
        "'''\n",
        "Below I've experimented with adding in dropout in each hidden layer and tested the effects of adding in\n",
        "another hidden layer. \n",
        "'''\n",
        "\n",
        "# Adding dropout to prevent over fitting. \n",
        "def build_classifier():\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    return classifier\n",
        "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 32, epochs = 100)\n",
        "accuracies = cross_val_score(estimator = classifier, X = x_train_processed , y = y_train, cv = 10, n_jobs = -1)\n",
        "mean = accuracies.mean()\n",
        "variance = accuracies.std()\n",
        "\n",
        "print(\"Adding in dropout the mean accuracy is now: {}\".format(mean))\n",
        "print(\"With variance of: {} \\n\".format(variance))\n",
        "\n",
        "# Adding in an additional hidden layer. \n",
        "def build_classifier():\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    return classifier\n",
        "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 32, epochs = 100)\n",
        "accuracies = cross_val_score(estimator = classifier, X = x_train_processed , y = y_train, cv = 10, n_jobs = -1)\n",
        "mean = accuracies.mean()\n",
        "variance = accuracies.std()\n",
        "\n",
        "print(\"Adding in dropout the mean accuracy is now: {}\".format(mean))\n",
        "print(\"With variance of: {} \\n\".format(variance))\n",
        "\n",
        "'''\n",
        "OUTPUT:\n",
        "Adding in dropout the mean accuracy is now: 0.7099567174911499\n",
        "With variance of: 0.07820415336432304 \n",
        "\n",
        "Adding in dropout the mean accuracy is now: 0.6958874583244323\n",
        "With variance of: 0.07884998617267996 \n",
        "\n",
        "From the output above we see the adding in dropout slightly decreases the mean accurancy, but this is \n",
        "likely worth it to prevent over fitting. \n",
        "\n",
        "Adding another hidden layer decreases the mean accuracy and increases the variance (very slightly) so, \n",
        "it's best not to add this new layer. \n",
        "\n",
        "So, we have finished tuning the model, though it's still not great, I have made a slight improvement.\n",
        "To make the model better, the main point of action would be to obtain more data!\n",
        "\n",
        "Below is the final and slightly improved model. \n",
        "'''\n",
        "\n",
        "#### Final Model #### \n",
        "\n",
        "# Initialising the ANN\n",
        "classifier = Sequential() # model class\n",
        "# Adding the input layer and the first hidden layer\n",
        "# add method used to add layers.\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13))\n",
        "# Adding dropout.\n",
        "classifier.add(Dropout(rate = 0.1))\n",
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "# Adding dropout.\n",
        "classifier.add(Dropout(rate = 0.1))\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "# Compiling the ANN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) \n",
        "\n",
        "# Dont change\n",
        "x_test_processed = my_model.preprocess_test_data(x_test)\n",
        "\n",
        "# Train your model\n",
        "# Fitting the ANN to the Training set\n",
        "classifier.fit(x_train_processed, y_train, batch_size = 32, epochs = 100)\n",
        "\n",
        "# use your model to make a prediction on unseen data\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(x_test_processed)\n",
        "y_pred = (y_pred > 0.5) # Threshold of 50% \n",
        "\n",
        "# Asssess the accuracy of your model and explain your key findings\n",
        "# Making the Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(cm)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\n",
        "print(\"Accuracy: {} \\n \\n\".format(accuracy))\n",
        "# Precision\n",
        "precision = (cm[1][1])/(cm[1][1]+cm[0][1])\n",
        "print(\"Precision: {} \\n \\n\".format(precision))\n",
        "# Recall \n",
        "recall = (cm[1][1])/(cm[1][1]+cm[1][0])\n",
        "print(\"Recall: {} \\n \\n\".format(recall))\n",
        "# F1 Score\n",
        "print(\"F1 Score: {} \\n \\n\".format((precision*recall)/(recall+precision)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LsUvejYjEjB"
      },
      "source": [
        "\"\"\"\n",
        "Testing on a single (made-up) value: \n",
        "age: 30-39\n",
        "menopause: ge40\n",
        "tumorsize: 25-29\n",
        "inv-nodes: 0-2\n",
        "node-caps: yes\n",
        "deg-malig: 3\n",
        "breast: right\n",
        "breast-quad: left_up\n",
        "irradiat: no\n",
        "\"\"\"\n",
        "# This can be encoded as: \n",
        "single_prediction = np.array([[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 6.0, 1.0, 1.0, 3.0, 1.0, 0.0]])\n",
        "\n",
        "# Predicting \n",
        "print(classifier.predict(single_prediction) > 0.5) \n",
        "# This gave a single value of \"True\" so, this individual is predicted to have a reccurance event. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pMldELDkgGy"
      },
      "source": [
        "aa = raw_data.iloc[:, :-1].values\n",
        "a = pd.DataFrame(my_model.preprocess_training_data(aa))\n",
        "\n",
        "print(a)\n",
        "print(a.iloc[286, :])\n",
        "\n",
        "print(y)\n",
        "#v = processed.iloc[287, :-1].values\n",
        "#print(v)\n",
        "#print(my_model.prediction_data([v]))\n",
        "\n",
        "#print(my_model.prediction_data())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fthMKg9hzcTd"
      },
      "source": [
        "# Version 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tWcKpczzhXF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4adfefbf-6060-4239-ca3c-409213edcae2"
      },
      "source": [
        "'''\n",
        "Deep Learning - Project\n",
        "\n",
        "Alistair Broad \n",
        "\n",
        "In the follow script I've used artificial neural networks to create a predictive model of \n",
        "whether breast cancer sufferers were likely to have recurrance. \n",
        "'''\n",
        "\n",
        "# Fixed dependencies - do not remove or change.\n",
        "import pytest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/gdrive/')\n",
        "# Import your dependencies\n",
        "\n",
        "# Dependencies \n",
        "import collections\n",
        "import datetime\n",
        "import keras\n",
        "from sklearn.preprocessing import OneHotEncoder # Encode the variables with more than one category that aren't ordinal. \n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras import Sequential\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "\n",
        "# Import data\n",
        "def import_local_data(file_path):\n",
        "    \"\"\"This function needs to import the data file into collab and return a pandas dataframe\n",
        "    \"\"\"\n",
        "    raw_df = pd.read_excel(file_path)\n",
        "\n",
        "\n",
        "    return raw_df\n",
        "\n",
        "\n",
        "local_file_path = '/content/drive/My Drive/Colab Notebooks/Data/breast-cancer.xls'\n",
        "\n",
        "# Dont change\n",
        "raw_data = import_local_data(local_file_path)\n",
        "\n",
        "\n",
        "# Exploring the Data\n",
        "\n",
        "# Inspect the dataset - gives simply view of the data set.\n",
        "print(\"Data looks like: \\n {}\".format(raw_data.head(10)), end =\"\\n \\n\")\n",
        "# Find the number of rows. \n",
        "print(\"The data has {} rows.\".format(len(raw_data.index)), end = \"\\n \\n\")\n",
        "# Iterate over the columns and inspect the unique values that occur and how many times they occur. \n",
        "for col in raw_data:  \n",
        "    print(\"Values in {} were:\".format(col), end = \"\\n\") \n",
        "    print(collections.Counter(raw_data[col]), end = \"\\n \\n\")\n",
        "\n",
        "\n",
        "print(\"Of the 286 observations, {}% of them were 'no-recurrence-events'.\".format(round((201/286)*100,1)), end = \"\\n \\n\")\n",
        "\n",
        "# Explain your key findings\n",
        "'''\n",
        "From a quick look at our data-set, we can see that there is 286 samples, which is a fairly small dataset for this sort of thing. \n",
        "We can also see that 70.3% were \"non-recurrance\" events, which may be important when considering/evaluating our final model. \n",
        "Now, looking at the data: \n",
        "Age - Median range is \"50-59\" and there's a skew towards older age ranges, \n",
        "Menopause - even though there's a skew towards older individuals, more were premenopause, \n",
        "tumor-size - to note here, \"10-14\" and \"5-9\" have been coverted in the source file to dates, \n",
        "inv-node - as with the above, \"3-5\", \"6-8\", \"9-11\" and \"12-14\" have also been converted to dates, \n",
        "node-caps - this has 8 missing data points and 78% answered \"no\",\n",
        "beast - this is fairly close to 50-50, as we would expect, \n",
        "breast-quad - this has 1 missing value.\n",
        "\n",
        "Also to note, is that the data contains a mix of data types (binary nominal, ordinal categorical etc.). \n",
        "\n",
        "We will need to find a suitable way to deal with missing values and encode the data so a solution can \n",
        "be found. \n",
        "'''\n",
        "\n",
        "# Split your data so that you can test the effectiveness of your model\n",
        "x = raw_data.iloc[:, :-1].values # Obtain the independent variables. \n",
        "y = raw_data.iloc[:, 9].values # Split out the dependent variable.\n",
        "\n",
        "# Encode the dependent variable.\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "\n",
        "# Creating the Training set and Test set\n",
        "# Here we take 25% of the data to test the model after learning from the other 80%.\n",
        "x_train, x_test, y_train, y_test = train_test_split( x, y, test_size = 0.25, random_state = 1) \n",
        "\n",
        "\n",
        "class Module4_Model:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        \n",
        "    def preprocess_training_data(self, training_df):\n",
        "        \"\"\"\n",
        "        This function should process the training data and store any features required in the class\n",
        "        \"\"\"\n",
        "        # Replacing Missing data\n",
        "        # For categorical data, we can delete row or replace with the mode.\n",
        "        # Below, I identify the required modes and sub in for missing values.\n",
        "\n",
        "        # Replacing missing values in \"node-caps\".\n",
        "        replace_missing = collections.Counter(training_df[:,4])            # This line counts the frequency of each entry.\n",
        "        replace_missing = replace_missing.most_common(1)[0][0]  # This finds the most frequent (mode).\n",
        "        training_df[:,4] = np.where(training_df[:,4] == '?', replace_missing, training_df[:,4]) # Applying the most common value to the missing ones. \n",
        "\n",
        "        # Replacing missing values in \"breast-quad\".\n",
        "        replace_missing = collections.Counter(training_df[:,7])            # This line counts the frequency of each entry.\n",
        "        replace_missing = replace_missing.most_common(1)[0][0]  # This finds the most frequent (mode).\n",
        "        training_df[:,7] = np.where(training_df[:,7] == '?', replace_missing, training_df[:,7]) # Applying the most common value to the missing ones. \n",
        "\n",
        "        training_df = pd.DataFrame(training_df) # Convert X to a dataframe to make the below easier.\n",
        "\n",
        "        # Encoding the Independent Variables\n",
        "        # ordinal categorical \n",
        "        age_mapping = {'20-29':1, '30-39':2, '40-49':3, '50-59':4, '60-69':5, '70-79':6}\n",
        "        training_df[0] = training_df[0].map(age_mapping)\n",
        "        tumorsize_mapping = { '0-4':1, datetime.datetime(2019, 9, 5, 0, 0):2, datetime.datetime(2014, 10, 1, 0, 0):3, '15-19':4, '20-24':5, '25-29':6, '30-34':7, '35-39':8, '40-44':9, '45-49':10, '50-54':11}\n",
        "        training_df[2] = training_df[2].map(tumorsize_mapping)\n",
        "        inv_nodes_mapping = {'0-2':1, datetime.datetime(2019, 5, 3, 0, 0):2, datetime.datetime(2019, 8, 6, 0, 0):3, datetime.datetime(2019, 11, 9, 0, 0):4, datetime.datetime(2014, 12, 1, 0, 0):5, '15-17':6, '24-26':7}\n",
        "        training_df[3] = training_df[3].map(inv_nodes_mapping)\n",
        "\n",
        "        # Nominal \n",
        "        node_caps_mapping = {\"no\":0, \"yes\":1}\n",
        "        training_df[4] = training_df[4].map(node_caps_mapping)\n",
        "        breast_mapping = {\"left\":0, \"right\":1}\n",
        "        training_df[6] = training_df[6].map(breast_mapping)\n",
        "        irradiat_mapping = {\"no\":0, \"yes\":1}\n",
        "        training_df[8] = training_df[8].map(irradiat_mapping)\n",
        "\n",
        "        # Encode the variables with more than one category that aren't ordinal. \n",
        "        ct = ColumnTransformer([('encoder', OneHotEncoder(), [1,7])], remainder='passthrough') \n",
        "        training_df = np.array(ct.fit_transform(training_df), dtype=np.float)\n",
        "        training_df = training_df[:, 1:] # Remove one dummy variable from menopause.\n",
        "        t = training_df[:, [2,0,1]]\n",
        "        training_df[:, [0,1,2]] = t\n",
        "        training_df = training_df[:, 1:] # Rearranging and removing 1 dummy variable for the \"breast-quad\" variable. \n",
        "\n",
        "        return training_df\n",
        "\n",
        "    def preprocess_test_data(self, test_df):\n",
        "        # Replacing Missing data\n",
        "        # For categorical data, we can delete row or replace with the mode.\n",
        "        # Below, I identify the required modes and sub in for missing values.\n",
        "\n",
        "        # Replacing missing values in \"node-caps\".\n",
        "        replace_missing = collections.Counter(test_df[:,4])            # This line counts the frequency of each entry.\n",
        "        replace_missing = replace_missing.most_common(1)[0][0]  # This finds the most frequent (mode).\n",
        "        test_df[:,4] = np.where(test_df[:,4] == '?', replace_missing, test_df[:,4]) # Applying the most common value to the missing ones. \n",
        "\n",
        "        # Replacing missing values in \"breast-quad\".\n",
        "        replace_missing = collections.Counter(test_df[:,7])            # This line counts the frequency of each entry.\n",
        "        replace_missing = replace_missing.most_common(1)[0][0]  # This finds the most frequent (mode).\n",
        "        test_df[:,7] = np.where(test_df[:,7] == '?', replace_missing, test_df[:,7]) # Applying the most common value to the missing ones. \n",
        "\n",
        "        test_df = pd.DataFrame(test_df) # Convert X to a dataframe to make the below easier.\n",
        "\n",
        "        # Encoding the Independent Variables\n",
        "        # ordinal categorical \n",
        "        age_mapping = {'20-29':1, '30-39':2, '40-49':3, '50-59':4, '60-69':5, '70-79':6}\n",
        "        test_df[0] = test_df[0].map(age_mapping)\n",
        "        tumorsize_mapping = { '0-4':1, datetime.datetime(2019, 9, 5, 0, 0):2, datetime.datetime(2014, 10, 1, 0, 0):3, '15-19':4, '20-24':5, '25-29':6, '30-34':7, '35-39':8, '40-44':9, '45-49':10, '50-54':11}\n",
        "        test_df[2] = test_df[2].map(tumorsize_mapping)\n",
        "        inv_nodes_mapping = {'0-2':1, datetime.datetime(2019, 5, 3, 0, 0):2, datetime.datetime(2019, 8, 6, 0, 0):3, datetime.datetime(2019, 11, 9, 0, 0):4, datetime.datetime(2014, 12, 1, 0, 0):5, '15-17':6, '24-26':7}\n",
        "        test_df[3] = test_df[3].map(inv_nodes_mapping)\n",
        "\n",
        "        # Nominal \n",
        "        node_caps_mapping = {\"no\":0, \"yes\":1}\n",
        "        test_df[4] = test_df[4].map(node_caps_mapping)\n",
        "        breast_mapping = {\"left\":0, \"right\":1}\n",
        "        test_df[6] = test_df[6].map(breast_mapping)\n",
        "        irradiat_mapping = {\"no\":0, \"yes\":1}\n",
        "        test_df[8] = test_df[8].map(irradiat_mapping)\n",
        "\n",
        "        # Encode the variables with more than one category that aren't ordinal. \n",
        "        ct = ColumnTransformer([('encoder', OneHotEncoder(), [1,7])], remainder='passthrough') \n",
        "        test_df = np.array(ct.fit_transform(test_df), dtype=np.float)\n",
        "        test_df = test_df[:, 1:] # Remove one dummy variable from menopause.\n",
        "        t = test_df[:, [2,0,1]]\n",
        "        test_df[:, [0,1,2]] = t\n",
        "        test_df = test_df[:, 1:] # Rearranging and removing 1 dummy variable for the \"breast-quad\" variable. \n",
        "\n",
        "        return test_df\n",
        "    \n",
        "\n",
        "# Dont change\n",
        "my_model = Module4_Model()\n",
        "\n",
        "# Dont change\n",
        "x_train_processed = my_model.preprocess_training_data(x_train)\n",
        "\n",
        "#### Model 1 #### \n",
        "\n",
        "# Create a model\n",
        "\n",
        "# Initialising the ANN\n",
        "classifier = Sequential() # model class\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "# add method used to add layers.\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13)) \n",
        "\n",
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu')) \n",
        "\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid')) # Sigmoid is used at the end to get probability at the end.\n",
        "# for more dependent variables with more than two categories use  - change units to the number you have and the activation to a multiple sigmoid version \"softmax\".\n",
        "\n",
        "# Compiling the ANN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) \n",
        "# adam is a stocastic gradient decent algorithm\n",
        "# for more dependent variables with more than two categories use \"category_crossentropy\"\n",
        "\n",
        "\n",
        "# Dont change\n",
        "x_test_processed = my_model.preprocess_test_data(x_test)\n",
        "\n",
        "\n",
        "# Train your model\n",
        "# Fitting the ANN to the Training set\n",
        "classifier.fit(x_train_processed, y_train, batch_size = 10, epochs = 100)\n",
        "\n",
        "# use your model to make a prediction on unseen data\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(x_test_processed)\n",
        "y_pred = (y_pred > 0.5) # Threshold of 50% \n",
        "\n",
        "# Asssess the accuracy of your model and explain your key findings\n",
        "# Making the Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(cm)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\n",
        "print(\"Accuracy: {} \\n \\n\".format(accuracy))\n",
        "# Precision\n",
        "precision = (cm[1][1])/(cm[1][1]+cm[0][1])\n",
        "print(\"Precision: {} \\n \\n\".format(precision))\n",
        "# Recall \n",
        "recall = (cm[1][1])/(cm[1][1]+cm[1][0])\n",
        "print(\"Recall: {} \\n \\n\".format(recall))\n",
        "# F1 Score\n",
        "print(\"F1 Score: {} \\n \\n\".format((precision*recall)/(recall+precision)))\n",
        "\n",
        "'''\n",
        "OUTPUT:\n",
        "[[46  7]\n",
        " [15  4]]\n",
        "Accuracy: 0.6944444444444444 \n",
        " \n",
        "\n",
        "Precision: 0.36363636363636365 \n",
        " \n",
        "\n",
        "Recall: 0.21052631578947367 \n",
        " \n",
        "\n",
        "F1 Score: 0.13333333333333333 \n",
        "\n",
        "Looking over these outputs, we can see that this is currently a pretty terrible model. Percision, recall and f1 are exceptionally low\n",
        "and accuracy is being below what would be achieved if the model always predicted \"non-recurrance\". Below I will try to impove the \n",
        "model by searching for the best parameters and dropout (which should help to prevent overfitting).\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "Below I've experimented with adding in dropout in each hidden layer and tested the effects of adding in\n",
        "another hidden layer. \n",
        "'''\n",
        "\n",
        "# Adding dropout to prevent over fitting. \n",
        "def build_classifier():\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    return classifier\n",
        "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 32, epochs = 100)\n",
        "accuracies = cross_val_score(estimator = classifier, X = x_train_processed , y = y_train, cv = 10, n_jobs = -1)\n",
        "mean = accuracies.mean()\n",
        "variance = accuracies.std()\n",
        "\n",
        "print(\"Adding in dropout the mean accuracy is now: {}\".format(mean))\n",
        "print(\"With variance of: {} \\n\".format(variance))\n",
        "\n",
        "# Adding in an additional hidden layer. \n",
        "def build_classifier():\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    return classifier\n",
        "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 32, epochs = 100)\n",
        "accuracies = cross_val_score(estimator = classifier, X = x_train_processed , y = y_train, cv = 10, n_jobs = -1)\n",
        "mean = accuracies.mean()\n",
        "variance = accuracies.std()\n",
        "\n",
        "print(\"Adding in dropout the mean accuracy is now: {}\".format(mean))\n",
        "print(\"With variance of: {} \\n\".format(variance))\n",
        "\n",
        "'''\n",
        "OUTPUT:\n",
        "Adding in dropout the mean accuracy is now: 0.7099567174911499\n",
        "With variance of: 0.07820415336432304 \n",
        "\n",
        "Adding in dropout the mean accuracy is now: 0.6958874583244323\n",
        "With variance of: 0.07884998617267996 \n",
        "\n",
        "From the output above we see the adding in dropout slightly decreases the mean accurancy, but this is \n",
        "likely worth it to prevent over fitting. \n",
        "\n",
        "Adding another hidden layer decreases the mean accuracy and increases the variance (very slightly) so, \n",
        "it's best not to add this new layer. \n",
        "\n",
        "So, we have finished tuning the model, though it's still not great, I have made a slight improvement.\n",
        "To make the model better, the main point of action would be to obtain more data!\n",
        "\n",
        "Below is the final and slightly improved model. \n",
        "'''\n",
        "\n",
        "#### Final Model #### \n",
        "\n",
        "# Initialising the ANN\n",
        "classifier = Sequential() # model class\n",
        "# Adding the input layer and the first hidden layer\n",
        "# add method used to add layers.\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13))\n",
        "# Adding dropout.\n",
        "classifier.add(Dropout(rate = 0.1))\n",
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "# Adding dropout.\n",
        "classifier.add(Dropout(rate = 0.1))\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "# Compiling the ANN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) \n",
        "\n",
        "# Dont change\n",
        "x_test_processed = my_model.preprocess_test_data(x_test)\n",
        "\n",
        "# Train your model\n",
        "# Fitting the ANN to the Training set\n",
        "classifier.fit(x_train_processed, y_train, batch_size = 32, epochs = 100)\n",
        "\n",
        "# use your model to make a prediction on unseen data\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(x_test_processed)\n",
        "y_pred = (y_pred > 0.5) # Threshold of 50% \n",
        "\n",
        "# Asssess the accuracy of your model and explain your key findings\n",
        "# Making the Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(cm)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\n",
        "print(\"Accuracy: {} \\n \\n\".format(accuracy))\n",
        "# Precision\n",
        "precision = (cm[1][1])/(cm[1][1]+cm[0][1])\n",
        "print(\"Precision: {} \\n \\n\".format(precision))\n",
        "# Recall \n",
        "recall = (cm[1][1])/(cm[1][1]+cm[1][0])\n",
        "print(\"Recall: {} \\n \\n\".format(recall))\n",
        "# F1 Score\n",
        "print(\"F1 Score: {} \\n \\n\".format((precision*recall)/(recall+precision)))\n",
        "\n",
        "\"\"\"\n",
        "Testing on a single (made-up) value: \n",
        "age: 30-39\n",
        "menopause: ge40\n",
        "tumorsize: 25-29\n",
        "inv-nodes: 0-2\n",
        "node-caps: yes\n",
        "deg-malig: 3\n",
        "breast: right\n",
        "breast-quad: left_up\n",
        "irradiat: no\n",
        "\"\"\"\n",
        "# This can be encoded as: \n",
        "single_prediction = np.array([[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 6.0, 1.0, 1.0, 3.0, 1.0, 0.0]])\n",
        "\n",
        "# Predicting \n",
        "print(classifier.predict(single_prediction) > 0.5) \n",
        "# This gave a single value of \"True\" so, this individual is predicted to have a reccurance event. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data looks like: \n",
            "      age menopause  ... irradiat                 Class\n",
            "0  40-49   premeno  ...       no     recurrence-events\n",
            "1  50-59      ge40  ...       no  no-recurrence-events\n",
            "2  50-59      ge40  ...       no     recurrence-events\n",
            "3  40-49   premeno  ...      yes  no-recurrence-events\n",
            "4  40-49   premeno  ...       no     recurrence-events\n",
            "5  50-59   premeno  ...      yes  no-recurrence-events\n",
            "6  50-59      ge40  ...       no  no-recurrence-events\n",
            "7  40-49   premeno  ...       no  no-recurrence-events\n",
            "8  40-49   premeno  ...       no  no-recurrence-events\n",
            "9  40-49      ge40  ...      yes  no-recurrence-events\n",
            "\n",
            "[10 rows x 10 columns]\n",
            " \n",
            "The data has 287 rows.\n",
            " \n",
            "Values in age were:\n",
            "Counter({'50-59': 96, '40-49': 90, '60-69': 57, '30-39': 37, '70-79': 6, '20-29': 1})\n",
            " \n",
            "Values in menopause were:\n",
            "Counter({'premeno': 150, 'ge40': 130, 'lt40': 7})\n",
            " \n",
            "Values in tumor-size were:\n",
            "Counter({'30-34': 60, '25-29': 55, '20-24': 50, '15-19': 30, datetime.datetime(2014, 10, 1, 0, 0): 28, '40-44': 22, '35-39': 19, '0-4': 8, '50-54': 8, datetime.datetime(2019, 9, 5, 0, 0): 4, '45-49': 3})\n",
            " \n",
            "Values in inv-nodes were:\n",
            "Counter({'0-2': 214, datetime.datetime(2019, 5, 3, 0, 0): 36, datetime.datetime(2019, 8, 6, 0, 0): 17, datetime.datetime(2019, 11, 9, 0, 0): 10, '15-17': 6, datetime.datetime(2014, 12, 1, 0, 0): 3, '24-26': 1})\n",
            " \n",
            "Values in node-caps were:\n",
            "Counter({'no': 222, 'yes': 57, '?': 8})\n",
            " \n",
            "Values in deg-malig were:\n",
            "Counter({2: 130, 3: 86, 1: 71})\n",
            " \n",
            "Values in breast were:\n",
            "Counter({'left': 152, 'right': 135})\n",
            " \n",
            "Values in breast-quad were:\n",
            "Counter({'left_low': 110, 'left_up': 98, 'right_up': 33, 'right_low': 24, 'central': 21, '?': 1})\n",
            " \n",
            "Values in irradiat were:\n",
            "Counter({'no': 219, 'yes': 68})\n",
            " \n",
            "Values in Class were:\n",
            "Counter({'no-recurrence-events': 202, 'recurrence-events': 85})\n",
            " \n",
            "Of the 286 observations, 70.3% of them were 'no-recurrence-events'.\n",
            " \n",
            "Epoch 1/100\n",
            "215/215 [==============================] - 0s 547us/step - loss: 0.6910 - accuracy: 0.6930\n",
            "Epoch 2/100\n",
            "215/215 [==============================] - 0s 122us/step - loss: 0.6853 - accuracy: 0.7023\n",
            "Epoch 3/100\n",
            "215/215 [==============================] - 0s 111us/step - loss: 0.6765 - accuracy: 0.7023\n",
            "Epoch 4/100\n",
            "215/215 [==============================] - 0s 113us/step - loss: 0.6621 - accuracy: 0.7023\n",
            "Epoch 5/100\n",
            "215/215 [==============================] - 0s 123us/step - loss: 0.6427 - accuracy: 0.7023\n",
            "Epoch 6/100\n",
            "215/215 [==============================] - 0s 141us/step - loss: 0.6282 - accuracy: 0.7023\n",
            "Epoch 7/100\n",
            "215/215 [==============================] - 0s 116us/step - loss: 0.6223 - accuracy: 0.7023\n",
            "Epoch 8/100\n",
            "215/215 [==============================] - 0s 118us/step - loss: 0.6195 - accuracy: 0.7023\n",
            "Epoch 9/100\n",
            "215/215 [==============================] - 0s 135us/step - loss: 0.6187 - accuracy: 0.7023\n",
            "Epoch 10/100\n",
            "215/215 [==============================] - 0s 135us/step - loss: 0.6168 - accuracy: 0.7023\n",
            "Epoch 11/100\n",
            "215/215 [==============================] - 0s 140us/step - loss: 0.6157 - accuracy: 0.7023\n",
            "Epoch 12/100\n",
            "215/215 [==============================] - 0s 130us/step - loss: 0.6139 - accuracy: 0.7023\n",
            "Epoch 13/100\n",
            "215/215 [==============================] - 0s 124us/step - loss: 0.6127 - accuracy: 0.7023\n",
            "Epoch 14/100\n",
            "215/215 [==============================] - 0s 136us/step - loss: 0.6120 - accuracy: 0.7023\n",
            "Epoch 15/100\n",
            "215/215 [==============================] - 0s 128us/step - loss: 0.6108 - accuracy: 0.7023\n",
            "Epoch 16/100\n",
            "215/215 [==============================] - 0s 140us/step - loss: 0.6093 - accuracy: 0.7023\n",
            "Epoch 17/100\n",
            "215/215 [==============================] - 0s 123us/step - loss: 0.6072 - accuracy: 0.7023\n",
            "Epoch 18/100\n",
            "215/215 [==============================] - 0s 127us/step - loss: 0.6059 - accuracy: 0.7023\n",
            "Epoch 19/100\n",
            "215/215 [==============================] - 0s 127us/step - loss: 0.6054 - accuracy: 0.7023\n",
            "Epoch 20/100\n",
            "215/215 [==============================] - 0s 123us/step - loss: 0.6028 - accuracy: 0.7023\n",
            "Epoch 21/100\n",
            "215/215 [==============================] - 0s 132us/step - loss: 0.6012 - accuracy: 0.7023\n",
            "Epoch 22/100\n",
            "215/215 [==============================] - 0s 124us/step - loss: 0.5997 - accuracy: 0.7023\n",
            "Epoch 23/100\n",
            "215/215 [==============================] - 0s 124us/step - loss: 0.5982 - accuracy: 0.7023\n",
            "Epoch 24/100\n",
            "215/215 [==============================] - 0s 137us/step - loss: 0.5956 - accuracy: 0.7023\n",
            "Epoch 25/100\n",
            "215/215 [==============================] - 0s 118us/step - loss: 0.5939 - accuracy: 0.7023\n",
            "Epoch 26/100\n",
            "215/215 [==============================] - 0s 127us/step - loss: 0.5920 - accuracy: 0.7023\n",
            "Epoch 27/100\n",
            "215/215 [==============================] - 0s 122us/step - loss: 0.5902 - accuracy: 0.7023\n",
            "Epoch 28/100\n",
            "215/215 [==============================] - 0s 128us/step - loss: 0.5885 - accuracy: 0.7023\n",
            "Epoch 29/100\n",
            "215/215 [==============================] - 0s 131us/step - loss: 0.5879 - accuracy: 0.7023\n",
            "Epoch 30/100\n",
            "215/215 [==============================] - 0s 154us/step - loss: 0.5864 - accuracy: 0.7023\n",
            "Epoch 31/100\n",
            "215/215 [==============================] - 0s 127us/step - loss: 0.5844 - accuracy: 0.7023\n",
            "Epoch 32/100\n",
            "215/215 [==============================] - 0s 134us/step - loss: 0.5835 - accuracy: 0.7023\n",
            "Epoch 33/100\n",
            "215/215 [==============================] - 0s 127us/step - loss: 0.5828 - accuracy: 0.7023\n",
            "Epoch 34/100\n",
            "215/215 [==============================] - 0s 121us/step - loss: 0.5815 - accuracy: 0.7023\n",
            "Epoch 35/100\n",
            "215/215 [==============================] - 0s 126us/step - loss: 0.5839 - accuracy: 0.7023\n",
            "Epoch 36/100\n",
            "215/215 [==============================] - 0s 125us/step - loss: 0.5808 - accuracy: 0.7023\n",
            "Epoch 37/100\n",
            "215/215 [==============================] - 0s 126us/step - loss: 0.5791 - accuracy: 0.7023\n",
            "Epoch 38/100\n",
            "215/215 [==============================] - 0s 122us/step - loss: 0.5792 - accuracy: 0.7023\n",
            "Epoch 39/100\n",
            "215/215 [==============================] - 0s 136us/step - loss: 0.5787 - accuracy: 0.7023\n",
            "Epoch 40/100\n",
            "215/215 [==============================] - 0s 115us/step - loss: 0.5773 - accuracy: 0.7023\n",
            "Epoch 41/100\n",
            "215/215 [==============================] - 0s 125us/step - loss: 0.5766 - accuracy: 0.7023\n",
            "Epoch 42/100\n",
            "215/215 [==============================] - 0s 128us/step - loss: 0.5763 - accuracy: 0.7023\n",
            "Epoch 43/100\n",
            "215/215 [==============================] - 0s 127us/step - loss: 0.5749 - accuracy: 0.7023\n",
            "Epoch 44/100\n",
            "215/215 [==============================] - 0s 139us/step - loss: 0.5735 - accuracy: 0.7023\n",
            "Epoch 45/100\n",
            "215/215 [==============================] - 0s 133us/step - loss: 0.5733 - accuracy: 0.7023\n",
            "Epoch 46/100\n",
            "215/215 [==============================] - 0s 133us/step - loss: 0.5724 - accuracy: 0.7023\n",
            "Epoch 47/100\n",
            "215/215 [==============================] - 0s 126us/step - loss: 0.5712 - accuracy: 0.7023\n",
            "Epoch 48/100\n",
            "215/215 [==============================] - 0s 114us/step - loss: 0.5707 - accuracy: 0.7023\n",
            "Epoch 49/100\n",
            "215/215 [==============================] - 0s 119us/step - loss: 0.5688 - accuracy: 0.7023\n",
            "Epoch 50/100\n",
            "215/215 [==============================] - 0s 127us/step - loss: 0.5686 - accuracy: 0.7023\n",
            "Epoch 51/100\n",
            "215/215 [==============================] - 0s 124us/step - loss: 0.5675 - accuracy: 0.7023\n",
            "Epoch 52/100\n",
            "215/215 [==============================] - 0s 128us/step - loss: 0.5666 - accuracy: 0.7023\n",
            "Epoch 53/100\n",
            "215/215 [==============================] - 0s 123us/step - loss: 0.5653 - accuracy: 0.7023\n",
            "Epoch 54/100\n",
            "215/215 [==============================] - 0s 115us/step - loss: 0.5643 - accuracy: 0.7023\n",
            "Epoch 55/100\n",
            "215/215 [==============================] - 0s 116us/step - loss: 0.5639 - accuracy: 0.7023\n",
            "Epoch 56/100\n",
            "215/215 [==============================] - 0s 132us/step - loss: 0.5641 - accuracy: 0.7023\n",
            "Epoch 57/100\n",
            "215/215 [==============================] - 0s 122us/step - loss: 0.5618 - accuracy: 0.7023\n",
            "Epoch 58/100\n",
            "215/215 [==============================] - 0s 142us/step - loss: 0.5618 - accuracy: 0.7023\n",
            "Epoch 59/100\n",
            "215/215 [==============================] - 0s 133us/step - loss: 0.5602 - accuracy: 0.7023\n",
            "Epoch 60/100\n",
            "215/215 [==============================] - 0s 136us/step - loss: 0.5594 - accuracy: 0.7023\n",
            "Epoch 61/100\n",
            "215/215 [==============================] - 0s 130us/step - loss: 0.5594 - accuracy: 0.7023\n",
            "Epoch 62/100\n",
            "215/215 [==============================] - 0s 130us/step - loss: 0.5580 - accuracy: 0.7023\n",
            "Epoch 63/100\n",
            "215/215 [==============================] - 0s 121us/step - loss: 0.5567 - accuracy: 0.7023\n",
            "Epoch 64/100\n",
            "215/215 [==============================] - 0s 127us/step - loss: 0.5560 - accuracy: 0.7023\n",
            "Epoch 65/100\n",
            "215/215 [==============================] - 0s 129us/step - loss: 0.5558 - accuracy: 0.7023\n",
            "Epoch 66/100\n",
            "215/215 [==============================] - 0s 126us/step - loss: 0.5547 - accuracy: 0.7023\n",
            "Epoch 67/100\n",
            "215/215 [==============================] - 0s 129us/step - loss: 0.5542 - accuracy: 0.7023\n",
            "Epoch 68/100\n",
            "215/215 [==============================] - 0s 124us/step - loss: 0.5545 - accuracy: 0.7023\n",
            "Epoch 69/100\n",
            "215/215 [==============================] - 0s 129us/step - loss: 0.5531 - accuracy: 0.7023\n",
            "Epoch 70/100\n",
            "215/215 [==============================] - 0s 125us/step - loss: 0.5527 - accuracy: 0.7023\n",
            "Epoch 71/100\n",
            "215/215 [==============================] - 0s 127us/step - loss: 0.5519 - accuracy: 0.7023\n",
            "Epoch 72/100\n",
            "215/215 [==============================] - 0s 124us/step - loss: 0.5521 - accuracy: 0.7023\n",
            "Epoch 73/100\n",
            "215/215 [==============================] - 0s 127us/step - loss: 0.5514 - accuracy: 0.7023\n",
            "Epoch 74/100\n",
            "215/215 [==============================] - 0s 135us/step - loss: 0.5502 - accuracy: 0.7023\n",
            "Epoch 75/100\n",
            "215/215 [==============================] - 0s 125us/step - loss: 0.5494 - accuracy: 0.7023\n",
            "Epoch 76/100\n",
            "215/215 [==============================] - 0s 130us/step - loss: 0.5496 - accuracy: 0.7023\n",
            "Epoch 77/100\n",
            "215/215 [==============================] - 0s 119us/step - loss: 0.5485 - accuracy: 0.7023\n",
            "Epoch 78/100\n",
            "215/215 [==============================] - 0s 127us/step - loss: 0.5499 - accuracy: 0.7023\n",
            "Epoch 79/100\n",
            "215/215 [==============================] - 0s 134us/step - loss: 0.5484 - accuracy: 0.7023\n",
            "Epoch 80/100\n",
            "215/215 [==============================] - 0s 121us/step - loss: 0.5471 - accuracy: 0.7023\n",
            "Epoch 81/100\n",
            "215/215 [==============================] - 0s 129us/step - loss: 0.5461 - accuracy: 0.7023\n",
            "Epoch 82/100\n",
            "215/215 [==============================] - 0s 182us/step - loss: 0.5460 - accuracy: 0.7023\n",
            "Epoch 83/100\n",
            "215/215 [==============================] - 0s 122us/step - loss: 0.5450 - accuracy: 0.7023\n",
            "Epoch 84/100\n",
            "215/215 [==============================] - 0s 122us/step - loss: 0.5474 - accuracy: 0.7023\n",
            "Epoch 85/100\n",
            "215/215 [==============================] - 0s 133us/step - loss: 0.5441 - accuracy: 0.7023\n",
            "Epoch 86/100\n",
            "215/215 [==============================] - 0s 128us/step - loss: 0.5460 - accuracy: 0.7023\n",
            "Epoch 87/100\n",
            "215/215 [==============================] - 0s 125us/step - loss: 0.5458 - accuracy: 0.7023\n",
            "Epoch 88/100\n",
            "215/215 [==============================] - 0s 127us/step - loss: 0.5424 - accuracy: 0.7023\n",
            "Epoch 89/100\n",
            "215/215 [==============================] - 0s 128us/step - loss: 0.5422 - accuracy: 0.7023\n",
            "Epoch 90/100\n",
            "215/215 [==============================] - 0s 140us/step - loss: 0.5422 - accuracy: 0.7023\n",
            "Epoch 91/100\n",
            "215/215 [==============================] - 0s 124us/step - loss: 0.5407 - accuracy: 0.7023\n",
            "Epoch 92/100\n",
            "215/215 [==============================] - 0s 127us/step - loss: 0.5412 - accuracy: 0.7023\n",
            "Epoch 93/100\n",
            "215/215 [==============================] - 0s 128us/step - loss: 0.5407 - accuracy: 0.7023\n",
            "Epoch 94/100\n",
            "215/215 [==============================] - 0s 120us/step - loss: 0.5403 - accuracy: 0.7023\n",
            "Epoch 95/100\n",
            "215/215 [==============================] - 0s 191us/step - loss: 0.5400 - accuracy: 0.7023\n",
            "Epoch 96/100\n",
            "215/215 [==============================] - 0s 130us/step - loss: 0.5393 - accuracy: 0.7023\n",
            "Epoch 97/100\n",
            "215/215 [==============================] - 0s 131us/step - loss: 0.5381 - accuracy: 0.7023\n",
            "Epoch 98/100\n",
            "215/215 [==============================] - 0s 138us/step - loss: 0.5379 - accuracy: 0.7023\n",
            "Epoch 99/100\n",
            "215/215 [==============================] - 0s 122us/step - loss: 0.5380 - accuracy: 0.7023\n",
            "Epoch 100/100\n",
            "215/215 [==============================] - 0s 125us/step - loss: 0.5366 - accuracy: 0.7023\n",
            "[[51  0]\n",
            " [21  0]]\n",
            "Accuracy: 0.7083333333333334 \n",
            " \n",
            "\n",
            "Precision: nan \n",
            " \n",
            "\n",
            "Recall: 0.0 \n",
            " \n",
            "\n",
            "F1 Score: nan \n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:251: RuntimeWarning: invalid value encountered in long_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Adding in dropout the mean accuracy is now: 0.7017316162586212\n",
            "With variance of: 0.05825527233621434 \n",
            "\n",
            "Adding in dropout the mean accuracy is now: 0.7017316162586212\n",
            "With variance of: 0.05825527233621434 \n",
            "\n",
            "Epoch 1/100\n",
            "215/215 [==============================] - 0s 721us/step - loss: 0.6926 - accuracy: 0.6558\n",
            "Epoch 2/100\n",
            "215/215 [==============================] - 0s 60us/step - loss: 0.6912 - accuracy: 0.7023\n",
            "Epoch 3/100\n",
            "215/215 [==============================] - 0s 66us/step - loss: 0.6897 - accuracy: 0.7023\n",
            "Epoch 4/100\n",
            "215/215 [==============================] - 0s 68us/step - loss: 0.6882 - accuracy: 0.7023\n",
            "Epoch 5/100\n",
            "215/215 [==============================] - 0s 65us/step - loss: 0.6865 - accuracy: 0.7023\n",
            "Epoch 6/100\n",
            "215/215 [==============================] - 0s 56us/step - loss: 0.6850 - accuracy: 0.7023\n",
            "Epoch 7/100\n",
            "215/215 [==============================] - 0s 61us/step - loss: 0.6829 - accuracy: 0.7023\n",
            "Epoch 8/100\n",
            "215/215 [==============================] - 0s 66us/step - loss: 0.6806 - accuracy: 0.7023\n",
            "Epoch 9/100\n",
            "215/215 [==============================] - 0s 75us/step - loss: 0.6780 - accuracy: 0.7023\n",
            "Epoch 10/100\n",
            "215/215 [==============================] - 0s 68us/step - loss: 0.6747 - accuracy: 0.7023\n",
            "Epoch 11/100\n",
            "215/215 [==============================] - 0s 76us/step - loss: 0.6721 - accuracy: 0.7023\n",
            "Epoch 12/100\n",
            "215/215 [==============================] - 0s 76us/step - loss: 0.6666 - accuracy: 0.7023\n",
            "Epoch 13/100\n",
            "215/215 [==============================] - 0s 105us/step - loss: 0.6616 - accuracy: 0.7023\n",
            "Epoch 14/100\n",
            "215/215 [==============================] - 0s 67us/step - loss: 0.6558 - accuracy: 0.7023\n",
            "Epoch 15/100\n",
            "215/215 [==============================] - 0s 67us/step - loss: 0.6519 - accuracy: 0.7023\n",
            "Epoch 16/100\n",
            "215/215 [==============================] - 0s 71us/step - loss: 0.6470 - accuracy: 0.7023\n",
            "Epoch 17/100\n",
            "215/215 [==============================] - 0s 68us/step - loss: 0.6405 - accuracy: 0.7023\n",
            "Epoch 18/100\n",
            "215/215 [==============================] - 0s 70us/step - loss: 0.6302 - accuracy: 0.7023\n",
            "Epoch 19/100\n",
            "215/215 [==============================] - 0s 61us/step - loss: 0.6313 - accuracy: 0.7023\n",
            "Epoch 20/100\n",
            "215/215 [==============================] - 0s 78us/step - loss: 0.6315 - accuracy: 0.7023\n",
            "Epoch 21/100\n",
            "215/215 [==============================] - 0s 69us/step - loss: 0.6268 - accuracy: 0.7023\n",
            "Epoch 22/100\n",
            "215/215 [==============================] - 0s 80us/step - loss: 0.6253 - accuracy: 0.7023\n",
            "Epoch 23/100\n",
            "215/215 [==============================] - 0s 67us/step - loss: 0.6270 - accuracy: 0.7023\n",
            "Epoch 24/100\n",
            "215/215 [==============================] - 0s 70us/step - loss: 0.6356 - accuracy: 0.7023\n",
            "Epoch 25/100\n",
            "215/215 [==============================] - 0s 60us/step - loss: 0.6205 - accuracy: 0.7023\n",
            "Epoch 26/100\n",
            "215/215 [==============================] - 0s 67us/step - loss: 0.6197 - accuracy: 0.7023\n",
            "Epoch 27/100\n",
            "215/215 [==============================] - 0s 74us/step - loss: 0.6244 - accuracy: 0.7023\n",
            "Epoch 28/100\n",
            "215/215 [==============================] - 0s 68us/step - loss: 0.6234 - accuracy: 0.7023\n",
            "Epoch 29/100\n",
            "215/215 [==============================] - 0s 71us/step - loss: 0.6248 - accuracy: 0.7023\n",
            "Epoch 30/100\n",
            "215/215 [==============================] - 0s 67us/step - loss: 0.6276 - accuracy: 0.7023\n",
            "Epoch 31/100\n",
            "215/215 [==============================] - 0s 71us/step - loss: 0.6211 - accuracy: 0.7023\n",
            "Epoch 32/100\n",
            "215/215 [==============================] - 0s 67us/step - loss: 0.6144 - accuracy: 0.7023\n",
            "Epoch 33/100\n",
            "215/215 [==============================] - 0s 90us/step - loss: 0.6173 - accuracy: 0.7023\n",
            "Epoch 34/100\n",
            "215/215 [==============================] - 0s 65us/step - loss: 0.6049 - accuracy: 0.7023\n",
            "Epoch 35/100\n",
            "215/215 [==============================] - 0s 76us/step - loss: 0.6101 - accuracy: 0.7023\n",
            "Epoch 36/100\n",
            "215/215 [==============================] - 0s 62us/step - loss: 0.6091 - accuracy: 0.7023\n",
            "Epoch 37/100\n",
            "215/215 [==============================] - 0s 67us/step - loss: 0.6173 - accuracy: 0.7023\n",
            "Epoch 38/100\n",
            "215/215 [==============================] - 0s 63us/step - loss: 0.6070 - accuracy: 0.7023\n",
            "Epoch 39/100\n",
            "215/215 [==============================] - 0s 65us/step - loss: 0.6046 - accuracy: 0.7023\n",
            "Epoch 40/100\n",
            "215/215 [==============================] - 0s 83us/step - loss: 0.5982 - accuracy: 0.7023\n",
            "Epoch 41/100\n",
            "215/215 [==============================] - 0s 61us/step - loss: 0.6069 - accuracy: 0.7023\n",
            "Epoch 42/100\n",
            "215/215 [==============================] - 0s 64us/step - loss: 0.6077 - accuracy: 0.7023\n",
            "Epoch 43/100\n",
            "215/215 [==============================] - 0s 69us/step - loss: 0.6016 - accuracy: 0.7023\n",
            "Epoch 44/100\n",
            "215/215 [==============================] - 0s 59us/step - loss: 0.6006 - accuracy: 0.7023\n",
            "Epoch 45/100\n",
            "215/215 [==============================] - 0s 68us/step - loss: 0.6009 - accuracy: 0.7023\n",
            "Epoch 46/100\n",
            "215/215 [==============================] - 0s 68us/step - loss: 0.6134 - accuracy: 0.7023\n",
            "Epoch 47/100\n",
            "215/215 [==============================] - 0s 82us/step - loss: 0.6076 - accuracy: 0.7023\n",
            "Epoch 48/100\n",
            "215/215 [==============================] - 0s 61us/step - loss: 0.6064 - accuracy: 0.7023\n",
            "Epoch 49/100\n",
            "215/215 [==============================] - 0s 65us/step - loss: 0.6081 - accuracy: 0.7023\n",
            "Epoch 50/100\n",
            "215/215 [==============================] - 0s 61us/step - loss: 0.6045 - accuracy: 0.7023\n",
            "Epoch 51/100\n",
            "215/215 [==============================] - 0s 81us/step - loss: 0.6024 - accuracy: 0.7023\n",
            "Epoch 52/100\n",
            "215/215 [==============================] - 0s 79us/step - loss: 0.5992 - accuracy: 0.7023\n",
            "Epoch 53/100\n",
            "215/215 [==============================] - 0s 65us/step - loss: 0.5942 - accuracy: 0.7023\n",
            "Epoch 54/100\n",
            "215/215 [==============================] - 0s 77us/step - loss: 0.5913 - accuracy: 0.7023\n",
            "Epoch 55/100\n",
            "215/215 [==============================] - 0s 68us/step - loss: 0.5967 - accuracy: 0.7023\n",
            "Epoch 56/100\n",
            "215/215 [==============================] - 0s 69us/step - loss: 0.5937 - accuracy: 0.7023\n",
            "Epoch 57/100\n",
            "215/215 [==============================] - 0s 72us/step - loss: 0.5988 - accuracy: 0.7023\n",
            "Epoch 58/100\n",
            "215/215 [==============================] - 0s 69us/step - loss: 0.5807 - accuracy: 0.7023\n",
            "Epoch 59/100\n",
            "215/215 [==============================] - 0s 77us/step - loss: 0.5830 - accuracy: 0.7023\n",
            "Epoch 60/100\n",
            "215/215 [==============================] - 0s 75us/step - loss: 0.5890 - accuracy: 0.7023\n",
            "Epoch 61/100\n",
            "215/215 [==============================] - 0s 88us/step - loss: 0.5818 - accuracy: 0.7023\n",
            "Epoch 62/100\n",
            "215/215 [==============================] - 0s 97us/step - loss: 0.5842 - accuracy: 0.7023\n",
            "Epoch 63/100\n",
            "215/215 [==============================] - 0s 67us/step - loss: 0.5959 - accuracy: 0.7023\n",
            "Epoch 64/100\n",
            "215/215 [==============================] - 0s 67us/step - loss: 0.5968 - accuracy: 0.7023\n",
            "Epoch 65/100\n",
            "215/215 [==============================] - 0s 61us/step - loss: 0.5939 - accuracy: 0.7023\n",
            "Epoch 66/100\n",
            "215/215 [==============================] - 0s 65us/step - loss: 0.5771 - accuracy: 0.7023\n",
            "Epoch 67/100\n",
            "215/215 [==============================] - 0s 75us/step - loss: 0.5934 - accuracy: 0.7023\n",
            "Epoch 68/100\n",
            "215/215 [==============================] - 0s 66us/step - loss: 0.5766 - accuracy: 0.7116\n",
            "Epoch 69/100\n",
            "215/215 [==============================] - 0s 63us/step - loss: 0.5803 - accuracy: 0.7116\n",
            "Epoch 70/100\n",
            "215/215 [==============================] - 0s 69us/step - loss: 0.5724 - accuracy: 0.7070\n",
            "Epoch 71/100\n",
            "215/215 [==============================] - 0s 65us/step - loss: 0.5754 - accuracy: 0.7256\n",
            "Epoch 72/100\n",
            "215/215 [==============================] - 0s 66us/step - loss: 0.5840 - accuracy: 0.7256\n",
            "Epoch 73/100\n",
            "215/215 [==============================] - 0s 57us/step - loss: 0.5735 - accuracy: 0.7349\n",
            "Epoch 74/100\n",
            "215/215 [==============================] - 0s 71us/step - loss: 0.5757 - accuracy: 0.7442\n",
            "Epoch 75/100\n",
            "215/215 [==============================] - 0s 81us/step - loss: 0.5785 - accuracy: 0.7302\n",
            "Epoch 76/100\n",
            "215/215 [==============================] - 0s 71us/step - loss: 0.5722 - accuracy: 0.7163\n",
            "Epoch 77/100\n",
            "215/215 [==============================] - 0s 67us/step - loss: 0.5714 - accuracy: 0.7302\n",
            "Epoch 78/100\n",
            "215/215 [==============================] - 0s 59us/step - loss: 0.5755 - accuracy: 0.7302\n",
            "Epoch 79/100\n",
            "215/215 [==============================] - 0s 67us/step - loss: 0.5780 - accuracy: 0.7395\n",
            "Epoch 80/100\n",
            "215/215 [==============================] - 0s 72us/step - loss: 0.5719 - accuracy: 0.7488\n",
            "Epoch 81/100\n",
            "215/215 [==============================] - 0s 92us/step - loss: 0.5763 - accuracy: 0.7395\n",
            "Epoch 82/100\n",
            "215/215 [==============================] - 0s 60us/step - loss: 0.5779 - accuracy: 0.7163\n",
            "Epoch 83/100\n",
            "215/215 [==============================] - 0s 55us/step - loss: 0.5902 - accuracy: 0.7116\n",
            "Epoch 84/100\n",
            "215/215 [==============================] - 0s 66us/step - loss: 0.5611 - accuracy: 0.7395\n",
            "Epoch 85/100\n",
            "215/215 [==============================] - 0s 63us/step - loss: 0.5885 - accuracy: 0.7302\n",
            "Epoch 86/100\n",
            "215/215 [==============================] - 0s 66us/step - loss: 0.5932 - accuracy: 0.7302\n",
            "Epoch 87/100\n",
            "215/215 [==============================] - 0s 68us/step - loss: 0.5664 - accuracy: 0.7349\n",
            "Epoch 88/100\n",
            "215/215 [==============================] - 0s 63us/step - loss: 0.5723 - accuracy: 0.7302\n",
            "Epoch 89/100\n",
            "215/215 [==============================] - 0s 63us/step - loss: 0.5761 - accuracy: 0.7256\n",
            "Epoch 90/100\n",
            "215/215 [==============================] - 0s 62us/step - loss: 0.5762 - accuracy: 0.7349\n",
            "Epoch 91/100\n",
            "215/215 [==============================] - 0s 67us/step - loss: 0.5969 - accuracy: 0.7256\n",
            "Epoch 92/100\n",
            "215/215 [==============================] - 0s 51us/step - loss: 0.5702 - accuracy: 0.7442\n",
            "Epoch 93/100\n",
            "215/215 [==============================] - 0s 56us/step - loss: 0.5766 - accuracy: 0.7209\n",
            "Epoch 94/100\n",
            "215/215 [==============================] - 0s 59us/step - loss: 0.5549 - accuracy: 0.7302\n",
            "Epoch 95/100\n",
            "215/215 [==============================] - 0s 66us/step - loss: 0.5702 - accuracy: 0.7209\n",
            "Epoch 96/100\n",
            "215/215 [==============================] - 0s 99us/step - loss: 0.5815 - accuracy: 0.7163\n",
            "Epoch 97/100\n",
            "215/215 [==============================] - 0s 62us/step - loss: 0.5710 - accuracy: 0.7349\n",
            "Epoch 98/100\n",
            "215/215 [==============================] - 0s 59us/step - loss: 0.5635 - accuracy: 0.7349\n",
            "Epoch 99/100\n",
            "215/215 [==============================] - 0s 57us/step - loss: 0.5707 - accuracy: 0.7302\n",
            "Epoch 100/100\n",
            "215/215 [==============================] - 0s 55us/step - loss: 0.5604 - accuracy: 0.7395\n",
            "[[51  0]\n",
            " [19  2]]\n",
            "Accuracy: 0.7361111111111112 \n",
            " \n",
            "\n",
            "Precision: 1.0 \n",
            " \n",
            "\n",
            "Recall: 0.09523809523809523 \n",
            " \n",
            "\n",
            "F1 Score: 0.08695652173913042 \n",
            " \n",
            "\n",
            "[[False]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guBYT9S8yKnM"
      },
      "source": [
        "# Assessment Final "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYN7v3-NyPLI"
      },
      "source": [
        "'''\n",
        "Deep Learning - Project\n",
        "\n",
        "Alistair Broad \n",
        "\n",
        "In the follow script I've used artificial neural networks to create a predictive model of \n",
        "whether breast cancer sufferers were likely to have recurrance. \n",
        "'''\n",
        "\n",
        "# Fixed dependencies - do not remove or change.\n",
        "import pytest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/gdrive/')\n",
        "# Import your dependencies\n",
        "\n",
        "# Dependencies \n",
        "import collections\n",
        "import datetime\n",
        "import keras\n",
        "from sklearn.preprocessing import OneHotEncoder # Encode the variables with more than one category that aren't ordinal. \n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras import Sequential\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "\n",
        "# Import data\n",
        "def import_local_data(file_path):\n",
        "    \"\"\"This function needs to import the data file into collab and return a pandas dataframe\n",
        "    \"\"\"\n",
        "    raw_df = pd.read_excel(file_path)\n",
        "\n",
        "\n",
        "    return raw_df\n",
        "\n",
        "\n",
        "local_file_path = '/content/drive/My Drive/Colab Notebooks/Data/breast-cancer.xls'\n",
        "\n",
        "# Dont change\n",
        "raw_data = import_local_data(local_file_path)\n",
        "\n",
        "\n",
        "# Exploring the Data\n",
        "\n",
        "# Inspect the dataset - gives simply view of the data set.\n",
        "print(\"Data looks like: \\n {}\".format(raw_data.head(10)), end =\"\\n \\n\")\n",
        "# Find the number of rows. \n",
        "print(\"The data has {} rows.\".format(len(raw_data.index)), end = \"\\n \\n\")\n",
        "# Iterate over the columns and inspect the unique values that occur and how many times they occur. \n",
        "for col in raw_data:  \n",
        "    print(\"Values in {} were:\".format(col), end = \"\\n\") \n",
        "    print(collections.Counter(raw_data[col]), end = \"\\n \\n\")\n",
        "\n",
        "\n",
        "print(\"Of the 286 observations, {}% of them were 'no-recurrence-events'.\".format(round((201/286)*100,1)), end = \"\\n \\n\")\n",
        "\n",
        "# Explain your key findings\n",
        "'''\n",
        "From a quick look at our data-set, we can see that there is 286 samples, which is a fairly small dataset for this sort of thing. \n",
        "We can also see that 70.3% were \"non-recurrance\" events, which may be important when considering/evaluating our final model. \n",
        "Now, looking at the data: \n",
        "Age - Median range is \"50-59\" and there's a skew towards older age ranges, \n",
        "Menopause - even though there's a skew towards older individuals, more were premenopause, \n",
        "tumor-size - to note here, \"10-14\" and \"5-9\" have been coverted in the source file to dates, \n",
        "inv-node - as with the above, \"3-5\", \"6-8\", \"9-11\" and \"12-14\" have also been converted to dates, \n",
        "node-caps - this has 8 missing data points and 78% answered \"no\",\n",
        "beast - this is fairly close to 50-50, as we would expect, \n",
        "breast-quad - this has 1 missing value.\n",
        "\n",
        "Also to note, is that the data contains a mix of data types (binary nominal, ordinal categorical etc.). \n",
        "\n",
        "We will need to find a suitable way to deal with missing values and encode the data so a solution can \n",
        "be found. \n",
        "'''\n",
        "\n",
        "# Split your data so that you can test the effectiveness of your model\n",
        "x = raw_data.iloc[:, :-1].values # Obtain the independent variables. \n",
        "y = raw_data.iloc[:, 9].values # Split out the dependent variable.\n",
        "\n",
        "# Encode the dependent variable.\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "\n",
        "# Creating the Training set and Test set\n",
        "# Here we take 25% of the data to test the model after learning from the other 75%.\n",
        "x_train, x_test, y_train, y_test = train_test_split( x, y, test_size = 0.25, random_state = 1) \n",
        "\n",
        "\n",
        "class Module4_Model:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        \n",
        "    def preprocess_training_data(self, training_df):\n",
        "        \"\"\"\n",
        "        This function should process the training data and store any features required in the class\n",
        "        \"\"\"\n",
        "        # Replacing Missing data\n",
        "        # For categorical data, we can delete row or replace with the mode.\n",
        "        # Below, I identify the required modes and sub in for missing values.\n",
        "\n",
        "        # Replacing missing values in \"node-caps\".\n",
        "        replace_missing = collections.Counter(training_df[:,4])            # This line counts the frequency of each entry.\n",
        "        replace_missing = replace_missing.most_common(1)[0][0]  # This finds the most frequent (mode).\n",
        "        training_df[:,4] = np.where(training_df[:,4] == '?', replace_missing, training_df[:,4]) # Applying the most common value to the missing ones. \n",
        "\n",
        "        # Replacing missing values in \"breast-quad\".\n",
        "        replace_missing = collections.Counter(training_df[:,7])            # This line counts the frequency of each entry.\n",
        "        replace_missing = replace_missing.most_common(1)[0][0]  # This finds the most frequent (mode).\n",
        "        training_df[:,7] = np.where(training_df[:,7] == '?', replace_missing, training_df[:,7]) # Applying the most common value to the missing ones. \n",
        "\n",
        "        training_df = pd.DataFrame(training_df) # Convert X to a dataframe to make the below easier.\n",
        "\n",
        "        # Encoding the Independent Variables\n",
        "        # ordinal categorical \n",
        "        age_mapping = {'20-29':1, '30-39':2, '40-49':3, '50-59':4, '60-69':5, '70-79':6}\n",
        "        training_df[0] = training_df[0].map(age_mapping)\n",
        "        tumorsize_mapping = { '0-4':1, datetime.datetime(2019, 9, 5, 0, 0):2, datetime.datetime(2014, 10, 1, 0, 0):3, '15-19':4, '20-24':5, '25-29':6, '30-34':7, '35-39':8, '40-44':9, '45-49':10, '50-54':11}\n",
        "        training_df[2] = training_df[2].map(tumorsize_mapping)\n",
        "        inv_nodes_mapping = {'0-2':1, datetime.datetime(2019, 5, 3, 0, 0):2, datetime.datetime(2019, 8, 6, 0, 0):3, datetime.datetime(2019, 11, 9, 0, 0):4, datetime.datetime(2014, 12, 1, 0, 0):5, '15-17':6, '24-26':7}\n",
        "        training_df[3] = training_df[3].map(inv_nodes_mapping)\n",
        "\n",
        "        # Nominal \n",
        "        node_caps_mapping = {\"no\":0, \"yes\":1}\n",
        "        training_df[4] = training_df[4].map(node_caps_mapping)\n",
        "        breast_mapping = {\"left\":0, \"right\":1}\n",
        "        training_df[6] = training_df[6].map(breast_mapping)\n",
        "        irradiat_mapping = {\"no\":0, \"yes\":1}\n",
        "        training_df[8] = training_df[8].map(irradiat_mapping)\n",
        "\n",
        "        # Encode the variables with more than one category that aren't ordinal. \n",
        "        ct = ColumnTransformer([('encoder', OneHotEncoder(), [1,7])], remainder='passthrough') \n",
        "        training_df = np.array(ct.fit_transform(training_df), dtype=np.float)\n",
        "        training_df = training_df[:, 1:] # Remove one dummy variable from menopause.\n",
        "        t = training_df[:, [2,0,1]]\n",
        "        training_df[:, [0,1,2]] = t\n",
        "        training_df = training_df[:, 1:] # Rearranging and removing 1 dummy variable for the \"breast-quad\" variable. \n",
        "\n",
        "        return training_df\n",
        "\n",
        "    def preprocess_test_data(self, test_df):\n",
        "        # Replacing Missing data\n",
        "        # For categorical data, we can delete row or replace with the mode.\n",
        "        # Below, I identify the required modes and sub in for missing values.\n",
        "\n",
        "        # Replacing missing values in \"node-caps\".\n",
        "        replace_missing = collections.Counter(test_df[:,4])            # This line counts the frequency of each entry.\n",
        "        replace_missing = replace_missing.most_common(1)[0][0]  # This finds the most frequent (mode).\n",
        "        test_df[:,4] = np.where(test_df[:,4] == '?', replace_missing, test_df[:,4]) # Applying the most common value to the missing ones. \n",
        "\n",
        "        # Replacing missing values in \"breast-quad\".\n",
        "        replace_missing = collections.Counter(test_df[:,7])            # This line counts the frequency of each entry.\n",
        "        replace_missing = replace_missing.most_common(1)[0][0]  # This finds the most frequent (mode).\n",
        "        test_df[:,7] = np.where(test_df[:,7] == '?', replace_missing, test_df[:,7]) # Applying the most common value to the missing ones. \n",
        "\n",
        "        test_df = pd.DataFrame(test_df) # Convert X to a dataframe to make the below easier.\n",
        "\n",
        "        # Encoding the Independent Variables\n",
        "        # ordinal categorical \n",
        "        age_mapping = {'20-29':1, '30-39':2, '40-49':3, '50-59':4, '60-69':5, '70-79':6}\n",
        "        test_df[0] = test_df[0].map(age_mapping)\n",
        "        tumorsize_mapping = { '0-4':1, datetime.datetime(2019, 9, 5, 0, 0):2, datetime.datetime(2014, 10, 1, 0, 0):3, '15-19':4, '20-24':5, '25-29':6, '30-34':7, '35-39':8, '40-44':9, '45-49':10, '50-54':11}\n",
        "        test_df[2] = test_df[2].map(tumorsize_mapping)\n",
        "        inv_nodes_mapping = {'0-2':1, datetime.datetime(2019, 5, 3, 0, 0):2, datetime.datetime(2019, 8, 6, 0, 0):3, datetime.datetime(2019, 11, 9, 0, 0):4, datetime.datetime(2014, 12, 1, 0, 0):5, '15-17':6, '24-26':7}\n",
        "        test_df[3] = test_df[3].map(inv_nodes_mapping)\n",
        "\n",
        "        # Nominal \n",
        "        node_caps_mapping = {\"no\":0, \"yes\":1}\n",
        "        test_df[4] = test_df[4].map(node_caps_mapping)\n",
        "        breast_mapping = {\"left\":0, \"right\":1}\n",
        "        test_df[6] = test_df[6].map(breast_mapping)\n",
        "        irradiat_mapping = {\"no\":0, \"yes\":1}\n",
        "        test_df[8] = test_df[8].map(irradiat_mapping)\n",
        "\n",
        "        # Encode the variables with more than one category that aren't ordinal. \n",
        "        ct = ColumnTransformer([('encoder', OneHotEncoder(), [1,7])], remainder='passthrough') \n",
        "        test_df = np.array(ct.fit_transform(test_df), dtype=np.float)\n",
        "        test_df = test_df[:, 1:] # Remove one dummy variable from menopause.\n",
        "        t = test_df[:, [2,0,1]]\n",
        "        test_df[:, [0,1,2]] = t\n",
        "        test_df = test_df[:, 1:] # Rearranging and removing 1 dummy variable for the \"breast-quad\" variable. \n",
        "\n",
        "        return test_df\n",
        "    \n",
        "\n",
        "# Dont change\n",
        "my_model = Module4_Model()\n",
        "\n",
        "# Dont change\n",
        "x_train_processed = my_model.preprocess_training_data(x_train)\n",
        "\n",
        "#### Model 1 #### \n",
        "\n",
        "# Create a model\n",
        "\n",
        "# Initialising the ANN\n",
        "classifier = Sequential() # model class\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "# add method used to add layers.\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13)) \n",
        "\n",
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu')) \n",
        "\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid')) # Sigmoid is used at the end to get probability at the end.\n",
        "# for more dependent variables with more than two categories use  - change units to the number you have and the activation to a multiple sigmoid version \"softmax\".\n",
        "\n",
        "# Compiling the ANN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) \n",
        "# adam is a stocastic gradient decent algorithm\n",
        "# for more dependent variables with more than two categories use \"category_crossentropy\"\n",
        "\n",
        "\n",
        "# Dont change\n",
        "x_test_processed = my_model.preprocess_test_data(x_test)\n",
        "\n",
        "\n",
        "# Train your model\n",
        "# Fitting the ANN to the Training set\n",
        "classifier.fit(x_train_processed, y_train, batch_size = 10, epochs = 100)\n",
        "\n",
        "# use your model to make a prediction on unseen data\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(x_test_processed)\n",
        "y_pred = (y_pred > 0.5) # Threshold of 50% \n",
        "\n",
        "# Asssess the accuracy of your model and explain your key findings\n",
        "# Making the Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(cm)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\n",
        "print(\"Accuracy: {} \\n \\n\".format(accuracy))\n",
        "# Precision\n",
        "precision = (cm[1][1])/(cm[1][1]+cm[0][1])\n",
        "print(\"Precision: {} \\n \\n\".format(precision))\n",
        "# Recall \n",
        "recall = (cm[1][1])/(cm[1][1]+cm[1][0])\n",
        "print(\"Recall: {} \\n \\n\".format(recall))\n",
        "# F1 Score\n",
        "print(\"F1 Score: {} \\n \\n\".format((precision*recall)/(recall+precision)))\n",
        "\n",
        "'''\n",
        "OUTPUT:\n",
        "[[46  7]\n",
        " [15  4]]\n",
        "Accuracy: 0.6944444444444444 \n",
        " \n",
        "\n",
        "Precision: 0.36363636363636365 \n",
        " \n",
        "\n",
        "Recall: 0.21052631578947367 \n",
        " \n",
        "\n",
        "F1 Score: 0.13333333333333333 \n",
        "\n",
        "Looking over these outputs, we can see that this is currently a pretty terrible model. Percision, recall and f1 are exceptionally low\n",
        "and accuracy is being below what would be achieved if the model always predicted \"non-recurrance\". Below I will try to impove the \n",
        "model by searching for the best parameters and dropout (which should help to prevent overfitting).\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "Parameter tuning with gridsearch \n",
        "Note: This section takes an exceptionally long time to run - hence, why I only used 200 epochs.\n",
        "\n",
        "Here I've tested if having more epochs, batch size or a different optimiser will give us a \n",
        "better model. \n",
        "'''\n",
        "\n",
        "# Tuning the ANN\n",
        "def build_classifier(optimizer):\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13))\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    return classifier\n",
        "classifier = KerasClassifier(build_fn = build_classifier)\n",
        "parameters = {'batch_size': [25, 32],\n",
        "              'epochs': [100, 200],\n",
        "              'optimizer': ['adam', 'rmsprop']}\n",
        "grid_search = GridSearchCV(estimator = classifier,\n",
        "                           param_grid = parameters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 10)\n",
        "grid_search = grid_search.fit(x_train_processed , y_train)\n",
        "best_parameters = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "\n",
        "print(best_accuracy)\n",
        "print(best_parameters)\n",
        "\n",
        "'''\n",
        "OUTPUT: \n",
        "0.7146245059288537\n",
        "{'batch_size': 32, 'epochs': 100, 'optimizer': 'adam'}\n",
        "\n",
        "So here we have found the the number of epochs and optimiser we'd chosen was the best, but this \n",
        "suggests that increasing batch size will give a marginally better result - the accuracy is now\n",
        "at least above the percentage of \"non-reccurance\".\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "Below I've experimented with adding in dropout in each hidden layer and tested the effects of adding in\n",
        "another hidden layer. \n",
        "'''\n",
        "\n",
        "# Adding dropout to prevent over fitting. \n",
        "def build_classifier():\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    return classifier\n",
        "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 32, epochs = 100)\n",
        "accuracies = cross_val_score(estimator = classifier, X = x_train_processed , y = y_train, cv = 10, n_jobs = -1)\n",
        "mean = accuracies.mean()\n",
        "variance = accuracies.std()\n",
        "\n",
        "print(\"Adding in dropout the mean accuracy is now: {}\".format(mean))\n",
        "print(\"With variance of: {} \\n\".format(variance))\n",
        "\n",
        "# Adding in an additional hidden layer. \n",
        "def build_classifier():\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    classifier.add(Dropout(rate = 0.1))\n",
        "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    return classifier\n",
        "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 32, epochs = 100)\n",
        "accuracies = cross_val_score(estimator = classifier, X = x_train_processed , y = y_train, cv = 10, n_jobs = -1)\n",
        "mean = accuracies.mean()\n",
        "variance = accuracies.std()\n",
        "\n",
        "print(\"Adding in dropout the mean accuracy is now: {}\".format(mean))\n",
        "print(\"With variance of: {} \\n\".format(variance))\n",
        "\n",
        "'''\n",
        "OUTPUT:\n",
        "Adding in dropout the mean accuracy is now: 0.7099567174911499\n",
        "With variance of: 0.07820415336432304 \n",
        "\n",
        "Adding in dropout the mean accuracy is now: 0.6958874583244323\n",
        "With variance of: 0.07884998617267996 \n",
        "\n",
        "From the output above we see the adding in dropout slightly decreases the mean accurancy, but this is \n",
        "likely worth it to prevent over fitting. \n",
        "\n",
        "Adding another hidden layer decreases the mean accuracy and increases the variance (very slightly) so, \n",
        "it's best not to add this new layer. \n",
        "\n",
        "So, we have finished tuning the model, though it's still not great, I have made a slight improvement.\n",
        "To make the model better, the main point of action would be to obtain more data!\n",
        "\n",
        "Below is the final and slightly improved model. \n",
        "'''\n",
        "\n",
        "#### Final Model #### \n",
        "\n",
        "# Initialising the ANN\n",
        "classifier = Sequential() # model class\n",
        "# Adding the input layer and the first hidden layer\n",
        "# add method used to add layers.\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13))\n",
        "# Adding dropout.\n",
        "classifier.add(Dropout(rate = 0.1))\n",
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "# Adding dropout.\n",
        "classifier.add(Dropout(rate = 0.1))\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "# Compiling the ANN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) \n",
        "\n",
        "# Dont change\n",
        "x_test_processed = my_model.preprocess_test_data(x_test)\n",
        "\n",
        "# Train your model\n",
        "# Fitting the ANN to the Training set\n",
        "classifier.fit(x_train_processed, y_train, batch_size = 32, epochs = 100)\n",
        "\n",
        "# use your model to make a prediction on unseen data\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(x_test_processed)\n",
        "y_pred = (y_pred > 0.5) # Threshold of 50% \n",
        "\n",
        "# Asssess the accuracy of your model and explain your key findings\n",
        "# Making the Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(cm)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\n",
        "print(\"Accuracy: {} \\n \\n\".format(accuracy))\n",
        "# Precision\n",
        "precision = (cm[1][1])/(cm[1][1]+cm[0][1])\n",
        "print(\"Precision: {} \\n \\n\".format(precision))\n",
        "# Recall \n",
        "recall = (cm[1][1])/(cm[1][1]+cm[1][0])\n",
        "print(\"Recall: {} \\n \\n\".format(recall))\n",
        "# F1 Score\n",
        "print(\"F1 Score: {} \\n \\n\".format((precision*recall)/(recall+precision)))\n",
        "\n",
        "\"\"\"\n",
        "Testing on a single (made-up) value: \n",
        "age: 30-39\n",
        "menopause: ge40\n",
        "tumorsize: 25-29\n",
        "inv-nodes: 0-2\n",
        "node-caps: yes\n",
        "deg-malig: 3\n",
        "breast: right\n",
        "breast-quad: left_up\n",
        "irradiat: no\n",
        "\"\"\"\n",
        "# This can be encoded as: \n",
        "single_prediction = np.array([[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 6.0, 1.0, 1.0, 3.0, 1.0, 0.0]])\n",
        "\n",
        "# Predicting \n",
        "print(classifier.predict(single_prediction) > 0.5) \n",
        "# This gave a single value of \"True\" so, this individual is predicted to have a reccurance event. "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}