{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning - Course.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1dvSlDpelrcjOE56jD82OdNyQCR-4jiJO",
      "authorship_tag": "ABX9TyPYs53uLhZZC7CFgxucrgbq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alouisbroad/Machine_Learning/blob/main/Deep_Learning_Course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRjwn_zlsOcb"
      },
      "source": [
        "# Section 1: Artificial Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLEEaoADyGQF"
      },
      "source": [
        "## ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weNA-sLgsNnK"
      },
      "source": [
        "# Artificial Neural Network\n",
        "\n",
        "# Installing Theano - runs computations. \n",
        "# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
        "\n",
        "# Installing Tensorflow - fast computations\n",
        "# pip install tensorflow\n",
        "\n",
        "# theano and tensorflow are used to make the actaully algorithms for deep learning - if you were to code them yourself.\n",
        "\n",
        "# Installing Keras\n",
        "# pip install --upgrade keras\n",
        "\n",
        "# Part 1 - Data Preprocessing\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Data/Churn_Modelling.csv')\n",
        "X = dataset.iloc[:, 3:13].values\n",
        "y = dataset.iloc[:, 13].values\n",
        "\n",
        "# Encoding categorical data - if needed.\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "labelencoder_X_2 = LabelEncoder()\n",
        "\n",
        "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
        "\n",
        "#onehotencoder = OneHotEncoder(categorical_features = [1])\n",
        "\n",
        "#X = onehotencoder.fit_transform(X).toarray()\n",
        "\n",
        "columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
        "\n",
        "X = np.array(columnTransformer.fit_transform(X), dtype = np.str)\n",
        "\n",
        "X = X[:, 1:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# Part 2 - Now let's make the ANN!\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Initialising the ANN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
        "\n",
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "# Compiling the ANN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the ANN to the Training set\n",
        "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n",
        "\n",
        "# Part 3 - Making predictions and evaluating the model\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw385E3Nyx9j"
      },
      "source": [
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PXKhb7oyI20"
      },
      "source": [
        "## ANN Homework Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxYsjidDyEmx"
      },
      "source": [
        "# Artificial Neural Network\n",
        "\n",
        "# Installing Theano\n",
        "# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
        "\n",
        "# Installing Tensorflow\n",
        "# pip install tensorflow\n",
        "\n",
        "# Installing Keras\n",
        "# pip install --upgrade keras\n",
        "\n",
        "# Part 1 - Data Preprocessing\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Data/Churn_Modelling.csv')\n",
        "X = dataset.iloc[:, 3:13].values\n",
        "y = dataset.iloc[:, 13].values\n",
        "\n",
        "# Encoding the Independent Variable\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "ct = ColumnTransformer([('encoder', OneHotEncoder(), [1,2])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X), dtype=np.float)\n",
        "X = X[:, 1:] \n",
        "t = X[:, [2,0,1]]\n",
        "X[:, [0,1,2]] = t\n",
        "X = X[:, 1:] # Rearranging and removing 1 dummy variable of each categorical variables\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# Part 2 - Now let's make the ANN!\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Initialising the ANN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
        "\n",
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "# Compiling the ANN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the ANN to the Training set\n",
        "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n",
        "\n",
        "# Part 3 - Making predictions and evaluating the model\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Predicting a single new observation\n",
        "\"\"\"Predict if the customer with the following informations will leave the bank:\n",
        "Geography: France\n",
        "Credit Score: 600\n",
        "Gender: Male\n",
        "Age: 40\n",
        "Tenure: 3\n",
        "Balance: 60000\n",
        "Number of Products: 2\n",
        "Has Credit Card: Yes\n",
        "Is Active Member: Yes\n",
        "Estimated Salary: 50000\"\"\"\n",
        "new_prediction = classifier.predict(sc.transform(np.array([[0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\n",
        "new_prediction = (new_prediction > 0.5)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzCrAoa5z-5k"
      },
      "source": [
        "print(cm)\n",
        "\n",
        "len(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xNXGua30oUL"
      },
      "source": [
        "## Evaluating, Improving and Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjqPfHH0bbUE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca3fb8fc-2bda-4b6d-98b7-02ce0775b39d"
      },
      "source": [
        "'''\n",
        "Bias accuracy trade off \n",
        "'''\n",
        "\n",
        "# Artificial Neural Network\n",
        "\n",
        "# Installing Theano\n",
        "# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
        "\n",
        "# Installing Tensorflow\n",
        "# pip install tensorflow\n",
        "\n",
        "# Installing Keras\n",
        "# pip install --upgrade keras\n",
        "\n",
        "# Part 1 - Data Preprocessing\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Data/Churn_Modelling.csv')\n",
        "X = dataset.iloc[:, 3:13].values\n",
        "y = dataset.iloc[:, 13].values\n",
        "\n",
        "# Encoding the Independent Variable\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "ct = ColumnTransformer([('encoder', OneHotEncoder(), [1,2])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X), dtype=np.float)\n",
        "X = X[:, 1:] \n",
        "t = X[:, [2,0,1]]\n",
        "X[:, [0,1,2]] = t\n",
        "X = X[:, 1:] # Rearranging and removing 1 dummy variable of each categorical variables\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "print(\"done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8gg_I4s9rIe"
      },
      "source": [
        "\n",
        "# Part 4 - Evaluating, Improving and Tuning the ANN\n",
        "\n",
        "# Evaluating the ANN\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "def build_classifier():\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    return classifier\n",
        "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 100)\n",
        "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs = -1)\n",
        "mean = accuracies.mean()\n",
        "variance = accuracies.std()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieovFwYieUWN"
      },
      "source": [
        "print(mean)\n",
        "print(variance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6gi8TFP0xl2"
      },
      "source": [
        "'''\n",
        "Dropout used to fix over fitting,\n",
        "\n",
        "over fitting will have higher acccuracy for training set than test set. \n",
        "\n",
        "Detect over fitting with a high variance when applying k-fold cross validation.\n",
        "'''\n",
        "# Part 2 - Now let's make the ANN!\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "# Initialising the ANN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
        "classifier.add(Dropout(p = 0.1))\n",
        "\n",
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "classifier.add(Dropout(p = 0.1))\n",
        "\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "# Compiling the ANN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the ANN to the Training set\n",
        "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n",
        "\n",
        "# Part 3 - Making predictions and evaluating the model\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Predicting a single new observation\n",
        "\"\"\"Predict if the customer with the following informations will leave the bank:\n",
        "Geography: France\n",
        "Credit Score: 600\n",
        "Gender: Male\n",
        "Age: 40\n",
        "Tenure: 3\n",
        "Balance: 60000\n",
        "Number of Products: 2\n",
        "Has Credit Card: Yes\n",
        "Is Active Member: Yes\n",
        "Estimated Salary: 50000\"\"\"\n",
        "new_prediction = classifier.predict(sc.transform(np.array([[0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\n",
        "new_prediction = (new_prediction > 0.5)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(cm)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKTpyuRW9uVE"
      },
      "source": [
        "'''\n",
        "Parameter tuning with gridsearch \n",
        "'''\n",
        "# Improving the ANN\n",
        "# Dropout Regularization to reduce overfitting if needed\n",
        "\n",
        "# Tuning the ANN\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "def build_classifier(optimizer):\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    return classifier\n",
        "classifier = KerasClassifier(build_fn = build_classifier)\n",
        "parameters = {'batch_size': [25, 32],\n",
        "              'epochs': [100, 500],\n",
        "              'optimizer': ['adam', 'rmsprop']}\n",
        "grid_search = GridSearchCV(estimator = classifier,\n",
        "                           param_grid = parameters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 10)\n",
        "grid_search = grid_search.fit(X_train, y_train)\n",
        "best_parameters = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMdYisTAkIe2"
      },
      "source": [
        "print(best_parameters)\n",
        "print(best_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8IHCzLtj5VK"
      },
      "source": [
        "# Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f30gybjskCT4"
      },
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "# Installing Theano\n",
        "# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
        "\n",
        "# Installing Tensorflow\n",
        "# pip install tensorflow\n",
        "\n",
        "# Installing Keras\n",
        "# pip install --upgrade keras\n",
        "\n",
        "# Part 1 - Building the CNN\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Initialising the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
        "\n",
        "# Step 2 - Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adding a second convolutional layer\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Step 4 - Full connection\n",
        "classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "# Compiling the CNN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Part 2 - Fitting the CNN to the images\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')\n",
        "\n",
        "classifier.fit_generator(training_set,\n",
        "                         steps_per_epoch = 8000,\n",
        "                         epochs = 25,\n",
        "                         validation_data = test_set,\n",
        "                         validation_steps = 2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAev4kVkxd7g"
      },
      "source": [
        "## Homework solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgYenTzwxTHI"
      },
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "# Installing Theano\n",
        "# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
        "\n",
        "# Installing Tensorflow\n",
        "# pip install tensorflow\n",
        "\n",
        "# Installing Keras\n",
        "# pip install --upgrade keras\n",
        "\n",
        "# Part 1 - Building the CNN\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "from tensorflow.python.keras.models import Sequential # sequence of layers\n",
        "from tensorflow.python.keras.layers import Conv2D     # Convolutional layers - 2D for images \n",
        "from tensorflow.python.keras.layers import MaxPooling2D # pooling for images\n",
        "from tensorflow.python.keras.layers import Flatten   # Flatten\n",
        "from tensorflow.python.keras.layers import Dense    # Add fully connected layers. \n",
        "\n",
        "\n",
        "# Initialising the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
        "\n",
        "# Step 2 - Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adding a second convolutional layer\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Step 4 - Full connection\n",
        "classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "# Compiling the CNN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Part 2 - Fitting the CNN to the images\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/My Drive/Colab Notebooks/dataset/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/My Drive/Colab Notebooks/dataset/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')\n",
        "\n",
        "classifier.fit_generator(training_set,\n",
        "                         steps_per_epoch = 8000,\n",
        "                         epochs = 15,\n",
        "                         validation_data = test_set,\n",
        "                         validation_steps = 2000)\n",
        "\n",
        "# Part 3 - Making new predictions\n",
        "\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('dataset/single_prediction/cat_or_dog_3.jpg', target_size = (64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = classifier.predict(test_image)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "    prediction = 'dog'\n",
        "else:\n",
        "    prediction = 'cat'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9xIdiflAfsW"
      },
      "source": [
        "# Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAzCeRI-Aopy"
      },
      "source": [
        "# Recurrent Neural Network\n",
        "\n",
        "\n",
        "\n",
        "# Part 1 - Data Preprocessing\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the training set\n",
        "dataset_train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Data/Google_Stock_Price_Train.csv')\n",
        "training_set = dataset_train.iloc[:, 1:2].values\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "training_set_scaled = sc.fit_transform(training_set)\n",
        "\n",
        "# Creating a data structure with 60 timesteps and 1 output\n",
        "X_train = []\n",
        "y_train = []\n",
        "for i in range(60, 1258):\n",
        "    X_train.append(training_set_scaled[i-60:i, 0])\n",
        "    y_train.append(training_set_scaled[i, 0])\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "# Reshaping\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "\n",
        "\n",
        "\n",
        "# Part 2 - Building the RNN\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "\n",
        "# Initialising the RNN\n",
        "regressor = Sequential()\n",
        "\n",
        "# Adding the first LSTM layer and some Dropout regularisation\n",
        "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Adding a second LSTM layer and some Dropout regularisation\n",
        "regressor.add(LSTM(units = 50, return_sequences = True))\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Adding a third LSTM layer and some Dropout regularisation\n",
        "regressor.add(LSTM(units = 50, return_sequences = True))\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Adding a fourth LSTM layer and some Dropout regularisation\n",
        "regressor.add(LSTM(units = 50))\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Adding the output layer\n",
        "regressor.add(Dense(units = 1))\n",
        "\n",
        "# Compiling the RNN\n",
        "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "\n",
        "# Fitting the RNN to the Training set\n",
        "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)\n",
        "\n",
        "\n",
        "\n",
        "# Part 3 - Making the predictions and visualising the results\n",
        "\n",
        "# Getting the real stock price of 2017\n",
        "dataset_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Data/Google_Stock_Price_Test.csv')\n",
        "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
        "\n",
        "# Getting the predicted stock price of 2017\n",
        "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n",
        "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
        "inputs = inputs.reshape(-1,1)\n",
        "inputs = sc.transform(inputs)\n",
        "X_test = []\n",
        "for i in range(60, 80):\n",
        "    X_test.append(inputs[i-60:i, 0])\n",
        "X_test = np.array(X_test)\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "predicted_stock_price = regressor.predict(X_test)\n",
        "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
        "\n",
        "# Visualising the results\n",
        "plt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price')\n",
        "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Google Stock Price')\n",
        "plt.title('Google Stock Price Prediction')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Google Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-Rp0hGGCCn3"
      },
      "source": [
        "# Self Organising Maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6ZJwTK9C2YE"
      },
      "source": [
        "from math import sqrt\n",
        "\n",
        "from numpy import (array, unravel_index, nditer, linalg, random, subtract,\n",
        "                   power, exp, pi, zeros, arange, outer, meshgrid, dot)\n",
        "from collections import defaultdict\n",
        "from warnings import warn\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Minimalistic implementation of the Self Organizing Maps (SOM).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def fast_norm(x):\n",
        "    \"\"\"Returns norm-2 of a 1-D numpy array.\n",
        "\n",
        "    * faster than linalg.norm in case of 1-D arrays (numpy 1.9.2rc1).\n",
        "    \"\"\"\n",
        "    return sqrt(dot(x, x.T))\n",
        "\n",
        "\n",
        "class MiniSom(object):\n",
        "    def __init__(self, x, y, input_len, sigma=1.0, learning_rate=0.5, decay_function=None, random_seed=None):\n",
        "        \"\"\"\n",
        "            Initializes a Self Organizing Maps.\n",
        "            x,y - dimensions of the SOM\n",
        "            input_len - number of the elements of the vectors in input\n",
        "            sigma - spread of the neighborhood function (Gaussian), needs to be adequate to the dimensions of the map.\n",
        "            (at the iteration t we have sigma(t) = sigma / (1 + t/T) where T is #num_iteration/2)\n",
        "            learning_rate - initial learning rate\n",
        "            (at the iteration t we have learning_rate(t) = learning_rate / (1 + t/T) where T is #num_iteration/2)\n",
        "            decay_function, function that reduces learning_rate and sigma at each iteration\n",
        "                            default function: lambda x,current_iteration,max_iter: x/(1+current_iteration/max_iter)\n",
        "            random_seed, random seed to use.\n",
        "        \"\"\"\n",
        "        if sigma >= x/2.0 or sigma >= y/2.0:\n",
        "            warn('Warning: sigma is too high for the dimension of the map.')\n",
        "        if random_seed:\n",
        "            self.random_generator = random.RandomState(random_seed)\n",
        "        else:\n",
        "            self.random_generator = random.RandomState(random_seed)\n",
        "        if decay_function:\n",
        "            self._decay_function = decay_function\n",
        "        else:\n",
        "            self._decay_function = lambda x, t, max_iter: x/(1+t/max_iter)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.sigma = sigma\n",
        "        self.weights = self.random_generator.rand(x,y,input_len)*2-1 # random initialization\n",
        "        for i in range(x):\n",
        "            for j in range(y):\n",
        "                self.weights[i,j] = self.weights[i,j] / fast_norm(self.weights[i,j]) # normalization\n",
        "        self.activation_map = zeros((x,y))\n",
        "        self.neigx = arange(x)\n",
        "        self.neigy = arange(y) # used to evaluate the neighborhood function\n",
        "        self.neighborhood = self.gaussian\n",
        "\n",
        "    def _activate(self, x):\n",
        "        \"\"\" Updates matrix activation_map, in this matrix the element i,j is the response of the neuron i,j to x \"\"\"\n",
        "        s = subtract(x, self.weights) # x - w\n",
        "        it = nditer(self.activation_map, flags=['multi_index'])\n",
        "        while not it.finished:\n",
        "            self.activation_map[it.multi_index] = fast_norm(s[it.multi_index])  # || x - w ||\n",
        "            it.iternext()\n",
        "\n",
        "    def activate(self, x):\n",
        "        \"\"\" Returns the activation map to x \"\"\"\n",
        "        self._activate(x)\n",
        "        return self.activation_map\n",
        "\n",
        "    def gaussian(self, c, sigma):\n",
        "        \"\"\" Returns a Gaussian centered in c \"\"\"\n",
        "        d = 2*pi*sigma*sigma\n",
        "        ax = exp(-power(self.neigx-c[0], 2)/d)\n",
        "        ay = exp(-power(self.neigy-c[1], 2)/d)\n",
        "        return outer(ax, ay)  # the external product gives a matrix\n",
        "\n",
        "    def diff_gaussian(self, c, sigma):\n",
        "        \"\"\" Mexican hat centered in c (unused) \"\"\"\n",
        "        xx, yy = meshgrid(self.neigx, self.neigy)\n",
        "        p = power(xx-c[0], 2) + power(yy-c[1], 2)\n",
        "        d = 2*pi*sigma*sigma\n",
        "        return exp(-p/d)*(1-2/d*p)\n",
        "\n",
        "    def winner(self, x):\n",
        "        \"\"\" Computes the coordinates of the winning neuron for the sample x \"\"\"\n",
        "        self._activate(x)\n",
        "        return unravel_index(self.activation_map.argmin(), self.activation_map.shape)\n",
        "\n",
        "    def update(self, x, win, t):\n",
        "        \"\"\"\n",
        "            Updates the weights of the neurons.\n",
        "            x - current pattern to learn\n",
        "            win - position of the winning neuron for x (array or tuple).\n",
        "            t - iteration index\n",
        "        \"\"\"\n",
        "        eta = self._decay_function(self.learning_rate, t, self.T)\n",
        "        sig = self._decay_function(self.sigma, t, self.T) # sigma and learning rate decrease with the same rule\n",
        "        g = self.neighborhood(win, sig)*eta # improves the performances\n",
        "        it = nditer(g, flags=['multi_index'])\n",
        "        while not it.finished:\n",
        "            # eta * neighborhood_function * (x-w)\n",
        "            self.weights[it.multi_index] += g[it.multi_index]*(x-self.weights[it.multi_index])\n",
        "            # normalization\n",
        "            self.weights[it.multi_index] = self.weights[it.multi_index] / fast_norm(self.weights[it.multi_index])\n",
        "            it.iternext()\n",
        "\n",
        "    def quantization(self, data):\n",
        "        \"\"\" Assigns a code book (weights vector of the winning neuron) to each sample in data. \"\"\"\n",
        "        q = zeros(data.shape)\n",
        "        for i, x in enumerate(data):\n",
        "            q[i] = self.weights[self.winner(x)]\n",
        "        return q\n",
        "\n",
        "    def random_weights_init(self, data):\n",
        "        \"\"\" Initializes the weights of the SOM picking random samples from data \"\"\"\n",
        "        it = nditer(self.activation_map, flags=['multi_index'])\n",
        "        while not it.finished:\n",
        "            self.weights[it.multi_index] = data[self.random_generator.randint(len(data))]\n",
        "            self.weights[it.multi_index] = self.weights[it.multi_index]/fast_norm(self.weights[it.multi_index])\n",
        "            it.iternext()\n",
        "\n",
        "    def train_random(self, data, num_iteration):\n",
        "        \"\"\" Trains the SOM picking samples at random from data \"\"\"\n",
        "        self._init_T(num_iteration)\n",
        "        for iteration in range(num_iteration):\n",
        "            rand_i = self.random_generator.randint(len(data)) # pick a random sample\n",
        "            self.update(data[rand_i], self.winner(data[rand_i]), iteration)\n",
        "\n",
        "    def train_batch(self, data, num_iteration):\n",
        "        \"\"\" Trains using all the vectors in data sequentially \"\"\"\n",
        "        self._init_T(len(data)*num_iteration)\n",
        "        iteration = 0\n",
        "        while iteration < num_iteration:\n",
        "            idx = iteration % (len(data)-1)\n",
        "            self.update(data[idx], self.winner(data[idx]), iteration)\n",
        "            iteration += 1\n",
        "\n",
        "    def _init_T(self, num_iteration):\n",
        "        \"\"\" Initializes the parameter T needed to adjust the learning rate \"\"\"\n",
        "        self.T = num_iteration/2  # keeps the learning rate nearly constant for the last half of the iterations\n",
        "\n",
        "    def distance_map(self):\n",
        "        \"\"\" Returns the distance map of the weights.\n",
        "            Each cell is the normalised sum of the distances between a neuron and its neighbours.\n",
        "        \"\"\"\n",
        "        um = zeros((self.weights.shape[0], self.weights.shape[1]))\n",
        "        it = nditer(um, flags=['multi_index'])\n",
        "        while not it.finished:\n",
        "            for ii in range(it.multi_index[0]-1, it.multi_index[0]+2):\n",
        "                for jj in range(it.multi_index[1]-1, it.multi_index[1]+2):\n",
        "                    if ii >= 0 and ii < self.weights.shape[0] and jj >= 0 and jj < self.weights.shape[1]:\n",
        "                        um[it.multi_index] += fast_norm(self.weights[ii, jj, :]-self.weights[it.multi_index])\n",
        "            it.iternext()\n",
        "        um = um/um.max()\n",
        "        return um\n",
        "\n",
        "    def activation_response(self, data):\n",
        "        \"\"\"\n",
        "            Returns a matrix where the element i,j is the number of times\n",
        "            that the neuron i,j have been winner.\n",
        "        \"\"\"\n",
        "        a = zeros((self.weights.shape[0], self.weights.shape[1]))\n",
        "        for x in data:\n",
        "            a[self.winner(x)] += 1\n",
        "        return a\n",
        "\n",
        "    def quantization_error(self, data):\n",
        "        \"\"\"\n",
        "            Returns the quantization error computed as the average distance between\n",
        "            each input sample and its best matching unit.\n",
        "        \"\"\"\n",
        "        error = 0\n",
        "        for x in data:\n",
        "            error += fast_norm(x-self.weights[self.winner(x)])\n",
        "        return error/len(data)\n",
        "\n",
        "    def win_map(self, data):\n",
        "        \"\"\"\n",
        "            Returns a dictionary wm where wm[(i,j)] is a list with all the patterns\n",
        "            that have been mapped in the position i,j.\n",
        "        \"\"\"\n",
        "        winmap = defaultdict(list)\n",
        "        for x in data:\n",
        "            winmap[self.winner(x)].append(x)\n",
        "        return winmap\n",
        "\n",
        "### unit tests\n",
        "from numpy.testing import assert_almost_equal, assert_array_almost_equal, assert_array_equal\n",
        "\n",
        "\n",
        "class TestMinisom:\n",
        "    def setup_method(self, method):\n",
        "        self.som = MiniSom(5, 5, 1)\n",
        "        for i in range(5):\n",
        "            for j in range(5):\n",
        "                assert_almost_equal(1.0, linalg.norm(self.som.weights[i,j]))  # checking weights normalization\n",
        "        self.som.weights = zeros((5, 5))  # fake weights\n",
        "        self.som.weights[2, 3] = 5.0\n",
        "        self.som.weights[1, 1] = 2.0\n",
        "\n",
        "    def test_decay_function(self):\n",
        "        assert self.som._decay_function(1., 2., 3.) == 1./(1.+2./3.)\n",
        "\n",
        "    def test_fast_norm(self):\n",
        "        assert fast_norm(array([1, 3])) == sqrt(1+9)\n",
        "\n",
        "    def test_gaussian(self):\n",
        "        bell = self.som.gaussian((2, 2), 1)\n",
        "        assert bell.max() == 1.0\n",
        "        assert bell.argmax() == 12  # unravel(12) = (2,2)\n",
        "\n",
        "    def test_win_map(self):\n",
        "        winners = self.som.win_map([5.0, 2.0])\n",
        "        assert winners[(2, 3)][0] == 5.0\n",
        "        assert winners[(1, 1)][0] == 2.0\n",
        "\n",
        "    def test_activation_reponse(self):\n",
        "        response = self.som.activation_response([5.0, 2.0])\n",
        "        assert response[2, 3] == 1\n",
        "        assert response[1, 1] == 1\n",
        "\n",
        "    def test_activate(self):\n",
        "        assert self.som.activate(5.0).argmin() == 13.0  # unravel(13) = (2,3)\n",
        "\n",
        "    def test_quantization_error(self):\n",
        "        self.som.quantization_error([5, 2]) == 0.0\n",
        "        self.som.quantization_error([4, 1]) == 0.5\n",
        "\n",
        "    def test_quantization(self):\n",
        "        q = self.som.quantization(array([4, 2]))\n",
        "        assert q[0] == 5.0\n",
        "        assert q[1] == 2.0\n",
        "\n",
        "    def test_random_seed(self):\n",
        "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
        "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
        "        assert_array_almost_equal(som1.weights, som2.weights)  # same initialization\n",
        "        data = random.rand(100,2)\n",
        "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
        "        som1.train_random(data,10)\n",
        "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
        "        som2.train_random(data,10)\n",
        "        assert_array_almost_equal(som1.weights,som2.weights)  # same state after training\n",
        "\n",
        "    def test_train_batch(self):\n",
        "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
        "        data = array([[4, 2], [3, 1]])\n",
        "        q1 = som.quantization_error(data)\n",
        "        som.train_batch(data, 10)\n",
        "        assert q1 > som.quantization_error(data)\n",
        "\n",
        "    def test_train_random(self):\n",
        "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
        "        data = array([[4, 2], [3, 1]])\n",
        "        q1 = som.quantization_error(data)\n",
        "        som.train_random(data, 10)\n",
        "        assert q1 > som.quantization_error(data)\n",
        "\n",
        "    def test_random_weights_init(self):\n",
        "        som = MiniSom(2, 2, 2, random_seed=1)\n",
        "        som.random_weights_init(array([[1.0, .0]]))\n",
        "        for w in som.weights:\n",
        "            assert_array_equal(w[0], array([1.0, .0]))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlEXUj1zC_VC"
      },
      "source": [
        "# Self Organizing Map\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Data/Credit_Card_Applications.csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "X = sc.fit_transform(X)\n",
        "\n",
        "# Training the SOM\n",
        "# from minisom import MiniSom\n",
        "som = MiniSom(x = 10, y = 10, input_len = 15, sigma = 1.0, learning_rate = 0.5)\n",
        "som.random_weights_init(X)\n",
        "som.train_random(data = X, num_iteration = 100)\n",
        "\n",
        "# Visualizing the results\n",
        "from pylab import bone, pcolor, colorbar, plot, show\n",
        "bone()\n",
        "pcolor(som.distance_map().T)\n",
        "colorbar()\n",
        "markers = ['o', 's']\n",
        "colors = ['r', 'g']\n",
        "for i, x in enumerate(X):\n",
        "    w = som.winner(x)\n",
        "    plot(w[0] + 0.5,\n",
        "         w[1] + 0.5,\n",
        "         markers[y[i]],\n",
        "         markeredgecolor = colors[y[i]],\n",
        "         markerfacecolor = 'None',\n",
        "         markersize = 10,\n",
        "         markeredgewidth = 2)\n",
        "show()\n",
        "\n",
        "# Finding the frauds\n",
        "mappings = som.win_map(X)\n",
        "frauds = np.concatenate((mappings[(8,1)], mappings[(6,8)]), axis = 0)\n",
        "frauds = sc.inverse_transform(frauds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyEJgKamCIU6"
      },
      "source": [
        "# Boltzmann Machines "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDvY92NzDQPN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lS-HffvCPbX"
      },
      "source": [
        "# AutoEncoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsHstphaDbb2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}